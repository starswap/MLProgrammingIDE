import numpy as np
import copy
ALPHABET = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz1234567890!'Â£$%^&*()[]-=+_;:@~#,<\n.>/?\\|\" {}`\t" #Each is a one hot token
N_TOKENS = len(ALPHABET)+2
HIDDEN_LAYER_NODES = 50 #Network architecture

def tanh(matrix):
  """Hyperbolic tangent"""
  matrix = matrix.astype(np.float64)
  return (np.exp(matrix) - np.exp(-matrix))/(np.exp(matrix)+np.exp(-matrix))

def softmax(z):
  """Softmax function - acts on vectors to normalise total sum to be out of 1, to create probabilities"""
  t = np.exp(z)
  return t/np.sum(t)

def autoCompleteNextLetter(a_prev,y_hat_prev,g_1=tanh):
    """Samples the next character of a novel sequence from the RNN for prediction"""

    z_t_1 = np.matmul(trainedW_a,np.vstack((a_prev,np.reshape(y_hat_prev,(y_hat_prev.shape[0],1))))) + trainedb_a # 100*1 
    a_prev = g_1(z_t_1) #100*1
    z_t_2 = np.matmul(trainedW_y,a_prev) + trainedb_y
    y_hat = softmax(z_t_2)

    return y_hat,a_prev

def suggestAutocomplete(preamble,cap=50,noTokens=N_TOKENS,hiddenLayerNodes=HIDDEN_LAYER_NODES):
    """Calls the autocompleteNextLetter() model repeatedly to sample novel sequences, hence creating autocomplete suggestions for the code the user has typed."""

    #MAINTENANCE: Change these to vary the number of first and second letters tried.
    #Example: With 5 and 2 you get 10 total completions, where the first 2 start with the most likely letter, second 2 start with next most common letter, and so on.
    #...every other letter chosen as just the maximum probability based on previously chosen letters.
    NUM_FIRST_LETTERS = 5 
    NUM_SECOND_LETTERS = 2

    #Initialisation
    y_hat = np.ones((noTokens,1)) #Predicted character to occur at this position    
    a_prev = np.zeros((hiddenLayerNodes,1)) #Will store the previous timestep activations to be passed into the next timestep
    y_hat,a_prev = autoCompleteNextLetter(a_prev,y_hat)
    oneToN = np.array(range(0,y_hat.shape[0])) #The numbers from 0 to the number of tokens-1 (i.e. index in alphabet. We will select one of these values based on the probabilities generated by the RNN) 


    #Ingest the start of the line before the completion we require
    for char in preamble:
        y_hat = np.zeros((noTokens,1)) #Create one hot vector for this character and use it as the previous time step output
        try:
            y_hat[ALPHABET.index(char)] = 1 #Make the correct one hot
        except ValueError: #Unknown char so use the unk token
            y_hat[noTokens-1] = 1

        y_hat,a_prev = autoCompleteNextLetter(a_prev,y_hat)

    #Choose only top NUM_FIRST_LETTERS first letters for completion strings
    #https://stackoverflow.com/questions/9007877/sort-arrays-rows-by-another-array-in-python
    indices = y_hat.T.argsort()
    order = oneToN[indices[0][::-1]]
    topF = order[:NUM_FIRST_LETTERS]

    #Will store a list of all possible completions - length is NUM_FIRST_LETTER * NUM_SECOND_LETTER
    results = []

    #Backup the activations so that we can use them later when generating the next sequence from the start. 
    firstCharAPrev = copy.deepcopy(a_prev)

    for firstChar in topF:
        #Generate probabilities for second character
        firstCharYHat = np.zeros((noTokens,1)) #Create one hot vector for this character and use it as the previous time step output
        firstCharYHat[firstChar] = 1 #Make the correct one hot
        y_hat,a_prev = autoCompleteNextLetter(firstCharAPrev,firstCharYHat)
        
        #Backup the activations again
        secondCharAPrev = copy.deepcopy(a_prev)
        
        #Choose only top NUM_SECOND_LETTERS second letters for completion strings.
        indices = y_hat.T.argsort()
        order = oneToN[indices[0][::-1]]
        topS = order[:NUM_SECOND_LETTERS]

        for secondChar in topS: #We generate a new completion for every first/second character combination
            try:
                currentResult = ALPHABET[firstChar] + ALPHABET[secondChar] #This variable will store the current completion value which will eventually be saved to the results array.
            except:
                currentResult = "U"
            count = 0 
            while currentResult[-1] != '\n' and len(currentResult) != cap: #until eol reached or string longer than max length at which point return
                #generate i th character probabilities
                if len(currentResult) == 2: #On the first iteration we should use the backed up activations after the first character has been chosen
                    secondCharyHat = np.zeros((noTokens,1)) #Create one hot vector for this character and use it as this time step's input
                    secondCharyHat[secondChar] = 1 #Make the correct one hot                
                    y_hat,a_prev = autoCompleteNextLetter(secondCharAPrev,secondCharyHat)
                else: #Otherwise we should use the activations from the previous layer.
                    y_hat,a_prev = autoCompleteNextLetter(a_prev,y_hat)

                indices = y_hat.T.argsort()
                order = oneToN[indices[0][::-1]]                   
                newChar = order[0]

                if newChar == noTokens-2: #Newline
                    currentResult += '\n'
                elif newChar == noTokens-1: #Unknown token - ignore
                    pass
                else: #Normal character - append
                     currentResult += ALPHABET[newChar] #successively build up string

            results.append(currentResult[:-1]) #save the current completion along with the others

    return [""] + results #Return possible completions back to user

#Load in the trained weights
trainedW_a = np.load("trainedW_a22.npy",allow_pickle=True)
trainedb_a = np.load("trainedb_a22.npy",allow_pickle=True)
trainedW_y = np.load("trainedW_y22.npy",allow_pickle=True)
trainedb_y = np.load("trainedb_y22.npy",allow_pickle=True)

