{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook: Readability Score",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LenvVgkhk0tB"
      },
      "source": [
        "## **Code Readability Score**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5DHVYAxbfZB"
      },
      "source": [
        "import numpy as np #Linear algebra library\n",
        "import os #Used to manipulate local file system to access the dataset data\n",
        "import copy #Used for copy.deepcopy to copy matrices to get new ones with the same dimensions \n",
        "import matplotlib.pyplot as plt #Used to plot the cost function against time for the network.\n",
        "import csv #Used to parse the csv files containing the data\n",
        "import re #Used to manipulate the snippets by splitting on punctuation in order to get identifiers out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ZaRFqspCMS"
      },
      "source": [
        "### Process Snippet Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SquQ9uwsGDua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9a2325-a1b2-4562-99dc-deb7ffdf6cfa"
      },
      "source": [
        "def SnippetsToFeatures():\n",
        "    \"\"\"Takes snippets from the Dorn readability dataset which are already saved locally and processes them into a set of machine learning features.\"\"\"\n",
        "    \n",
        "    X_DIRECTORY = \"/content/drive/MyDrive/A Level/Computer Science/NEA/Readability Data\"  #The local directory containing all of the snippets\n",
        "    Y_FILE = \"/content/drive/MyDrive/A Level/Computer Science/NEA/Readability Data/python.csv\" #The csv file containing ratings for each snippet\n",
        "\n",
        "    #Initialise matrices for total rating and no of times rated (to compute averages)\n",
        "    y = np.zeros((1,len(os.listdir(X_DIRECTORY))-1))\n",
        "    noRatings = np.zeros((1,len(os.listdir(X_DIRECTORY))-1))\n",
        "\n",
        "    #Open the csv file and iterate over it\n",
        "    with open(Y_FILE) as csv_file: #Source https://realpython.com/python-csv/\n",
        "        yData = csv.reader(csv_file, delimiter=',')\n",
        "        for row in yData: #Each row represents one person who rated snippets; each column is one snippet. Each row contains a large number of blank cells (unrated snippets) and a few data cells\n",
        "            for i,value in enumerate(row[1:]): #for each snippet the user rated (skipping first column which is uid)\n",
        "                if value != \"\": #if the user did rate the snippet\n",
        "                    y[0][i] += int(value) #add the rating and count the number of ratings so we can average later\n",
        "                    noRatings[0][i] += 1\n",
        "    \n",
        "    y /= noRatings #average\n",
        "    \n",
        "    #The ratings were 1 to 5 inclusive. Convert to be from 0 to 10 which is what we want for our output\n",
        "    y -= 1 \n",
        "    y *= 10/4\n",
        "\n",
        "    #Now we have all of the average ratings, for every snipppet get the snippet and extract features and then put the snippet next to its rating in the matrix\n",
        "    for i,file in enumerate(os.listdir(X_DIRECTORY)): #for every file in the directory that stores them\n",
        "        if os.path.splitext(file)[1] == '.jsnp': #if the file has the correct extension it must be a snippet file\n",
        "            with open(os.path.join(X_DIRECTORY,file)) as f: #read the file contents before passing it into the ExtractCodeReadabilityFeatures routine\n",
        "                if i == 0: #For the first one we need to create the dataMatrix variable as the result ...\n",
        "                    dataMatrix = ExtractCodeReadabilityFeatures(f.read())\n",
        "                else:\n",
        "                    dataMatrix = np.hstack((dataMatrix,ExtractCodeReadabilityFeatures(f.read()))) #... and for subsequent ones we need to stack the results horizontally next to the existing dataMatrix\n",
        "    dataMatrix = np.vstack((dataMatrix,y)) #Now put the y values on the bottom so each row represents an x or y feature and each column represents one input snippet training example.\n",
        "    \n",
        "    return dataMatrix #output data matrix back to caller.\n",
        "\n",
        "def ExtractCodeReadabilityFeatures(codeSnippet):\n",
        "    \"\"\"Takes a code snippet and converts it to a matrix of statistical features describing it, which will be used to score it on readability.\"\"\"\n",
        "    \n",
        "    #These are used to determine whether a word seen in the code snippet is an identifier or a language keyword\n",
        "    LANGUAGE_KEYWORDS = [\"for\",\"if\",\"def\",\"lambda\",\"while\",\"await\",\"else\",\"elif\",\"import\",\"pass\",\"break\",\"except\",\"in\",\"raise\",\"class\",\"finally\",\"is\",\"return\",\"and\",\"continue\",\"try\",\"as\",\"from\",\"assert\",\"del\",\"global\",\"not\",\"with\",\"async\",\"or\",\"yield\",\"self\",\"False\",\"True\",\"AttributeError\",\"None\"] #Source https://www.programiz.com/python-programming/keyword-list \n",
        "    \n",
        "    #MAINTENANCE: To change the designated tab style (tabs/spaces) for this part of the solution, swap the commented line\n",
        "    TAB_CHAR = \"    \"\n",
        " #   TAB_CHAR = \"\\t\" \n",
        "    \n",
        "    #Initialise accumulator variables counting each feature which will later be averaged.\n",
        "    numberOfLines = 0    \n",
        "    totalLineLength = 0\n",
        "    totalEmptyLines = 0\n",
        "    maxIndentDepth = 0\n",
        "    totalDigits = 0\n",
        "    totalSpaces = 0\n",
        "    prevIndent = 0\n",
        "    blockLength = 0\n",
        "    totalBlockLength = 0 \n",
        "    totalBlocks = 0 \n",
        "    totalIdentifiers = 0\n",
        "    totalIdentifierLength = 0\n",
        "    totalCamelPascal = 0\n",
        "    totalUnder = 0\n",
        "    totalUpperCase = 0 \n",
        "    idCounts = {} #This dictionary counts occurrences of each identifier in the code snippet so we can later calculate things like the number of occurrences of the most frequent identifier\n",
        "\n",
        "    for line in codeSnippet.split(\"\\n\"): #Go over every line in the snippet\n",
        "        numberOfLines += 1 #Count lines\n",
        "        totalLineLength += len(line) #This variable will later be divided by numberOfLines to get averageLineLength\n",
        "        if line == \"\": #Count empty lines\n",
        "            totalEmptyLines += 1\n",
        "            continue #Skip further processing on empty lines\n",
        "\n",
        "        if line.count(TAB_CHAR) > maxIndentDepth: #One of the features is the maximum indentation depth in tabs (or multiples of 4 spaces)\n",
        "            maxIndentDepth = line.count(TAB_CHAR)\n",
        "        \n",
        "        totalDigits += sum([line.count(a) for a in \"1234567890\"]) #Count all digits in the line\n",
        "        totalSpaces += line.count(\" \") #Count all spaces in the line\n",
        "\n",
        "        if line.count(TAB_CHAR) != prevIndent: #prevIndent stores the indent level of the previous line, so that we know if indentation has changed (if yes we are on a new block of code)\n",
        "            totalBlockLength += blockLength #We will later divide this by total blocks to get average block length\n",
        "            blockLength = 0\n",
        "            totalBlocks += 1\n",
        "        prevIndent = line.count(TAB_CHAR) #this line's indent is saved for next time\n",
        "        blockLength += 1 \n",
        "\n",
        "        #We take the part of the current line up to any comment (# or \"\"\") and split it on punctuation, yielding only \"words\". These are either identifiers or keywords\n",
        "        for item in re.split(\"\\W+\",line.split(\"#\")[0].split(\"\\\"\\\"\\\"\")[0]): #https://stackoverflow.com/questions/1059559/split-strings-into-words-with-multiple-word-boundary-delimiters\n",
        "            if item == \"\": #There were two symbols in a row, yielding a blank match which we ignore\n",
        "                continue\n",
        "            if not(item in LANGUAGE_KEYWORDS): #Not a language keyword therefore...\n",
        "                #item is an identifier name\n",
        "                totalIdentifiers += 1 #Count identifiers\n",
        "                totalIdentifierLength += len(item) #Will be used to get average identifier length\n",
        "                \n",
        "                #Count different variable naming conventions\n",
        "                if \"_\" in item: #underscore_based\n",
        "                    totalUnder += 1\n",
        "                if re.search(\"[A-Z]\",item) and re.search(\"[a-z]\",item): #camelCase/PascalCase\n",
        "                    totalCamelPascal += 1\n",
        "                elif re.search(\"[A-Z]\",item): #UPPER CASE\n",
        "                    totalUpperCase += 1\n",
        "                \n",
        "                #Increment the number of times we have seen this identifier in the code so we can later find the maximum occurrences of a single identifier\n",
        "                if item in idCounts:\n",
        "                    idCounts[item] = idCounts[item] + 1 \n",
        "                else:\n",
        "                    idCounts[item] = 1\n",
        "    \n",
        "    totalSharpComments = codeSnippet.count(\"#\")\n",
        "\n",
        "    #Avoid divide by 0 errors\n",
        "    if totalIdentifiers == 0:\n",
        "        totalIdentifiers = 1\n",
        "    if totalBlocks == 0:\n",
        "        totalBlocks = 1\n",
        "\n",
        "    #Compute averages\n",
        "    averageLineLength = totalLineLength/numberOfLines\n",
        "    averageIdentifierLength = totalIdentifierLength/totalIdentifiers\n",
        "    averageIdPerLine = totalIdentifiers/numberOfLines\n",
        "    averageSharpCommentsPerLine = totalSharpComments/numberOfLines\n",
        "    averageEmptyLinesPerLine = totalEmptyLines/numberOfLines\n",
        "    averageCamelPascalPerLine = totalCamelPascal/numberOfLines\n",
        "    averageUnderPerLine = totalUnder/numberOfLines\n",
        "    averageCapsPerLine = totalUpperCase/numberOfLines\n",
        "    averageDigitsPerLine = totalDigits/numberOfLines\n",
        "    averageSpacesPerLine = totalSpaces/numberOfLines\n",
        "    averageBlockLength = totalBlockLength/totalBlocks\n",
        "\n",
        "    #Get maximum occurrence of any identifier\n",
        "    if len(idCounts) > 0: #If we have at least one identifier, sort the identifiers and then get the last (maximum as ascending) one\n",
        "        maximumOccurOfID = idCounts[sorted(idCounts, key=idCounts.get)[-1]] #Source: https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
        "    else:\n",
        "        maximumOccurOfID = 0 #There were no identifiers. We deal with this case separately because otherwise we get an index error on the [-1] above\n",
        "    \n",
        "    #Count remaining things to count and average over number of lines so we don't penalise long code\n",
        "    #   List of regular expressions to match e.g. comparison operators, bitwise operators, assignment,  arithmetic\n",
        "    THINGS_TO_LOOK_FOR = [\"<|>|==|<=|>=\",\"\\^|\\~|\\&|\\|\",\"[^=]=[^=]\",\"\\+|\\-|\\/|\\*\",\"\\,\",\"\\.\",\"for\",\"if|elif|else\",\"def\",\"lambda\",\"while\",\"pass\",\"break\",\"except\",\"in\",\"raise\",\"class\",\"finally\",\"is\",\"return\",\"and\",\"continue\",\"try\",\"as\",\"from\",\"assert\",\"del\",\"global\",\"not\",\"with\",\"async\",\"or\",\"yield\",\"\\@\",\"print\"]\n",
        "    ##THINGS_TO_LOOK_FOR = [\"<|>|==|<=|>=\",\"\\^|\\~|\\&|\\|\",\"[^=]=[^=]\",\"\\+|\\-|\\/|\\*\",\"\\,\",\"\\.\",\"for\",\"if|elif|else\",\"def\",\"lambda\",\"while\",\"pass\",\"break|continue\",\"try|except|finally\",\"raise\",\"class\",\"assert\",\"global\",\"with\"]\n",
        "    #THINGS_TO_LOOK_FOR = [\"<|>|==|<=|>=\",\"[^=]=[^=]\",\"\\+|\\-|\\/|\\*\",\"\\,\",\"\\.\",\"for\",\"if|elif|else\",\"def\"]#\"lambda\",\"while\"] #,\"pass\",\"break\",\"except\",\"in\",\"raise\",\"class\",\"finally\",\"is\",\"return\",\"and\",\"continue\",\"try\",\"as\",\"from\",\"assert\",\"del\",\"global\",\"not\",\"with\",\"async\",\"or\",\"yield\",\"\\@\",\"print\"]\n",
        "\n",
        "    #Initialise the matrix of features we will store everything in\n",
        "    N_FEATURES = 13 + len(THINGS_TO_LOOK_FOR)\n",
        "    xFeatures = np.zeros((N_FEATURES,1))\n",
        "\n",
        "    for i,item in enumerate(THINGS_TO_LOOK_FOR): #Search for each thing and fill up the features matrix\n",
        "        xFeatures[i][0] = len(re.findall(item, codeSnippet))/numberOfLines #One feature is the amount of times each of the THINGS_TO_LOOK_FOR appears in the code, on average per line \n",
        "    \n",
        "    #Save remaining features\n",
        "    xFeatures[i+1] = averageLineLength\n",
        "    xFeatures[i+2] = averageIdentifierLength\n",
        "    xFeatures[i+3] = averageIdPerLine\n",
        "    xFeatures[i+4] = averageSharpCommentsPerLine\n",
        "    xFeatures[i+5] = averageEmptyLinesPerLine\n",
        "    xFeatures[i+6] = averageCamelPascalPerLine\n",
        "    xFeatures[i+7] = averageUnderPerLine\n",
        "    xFeatures[i+8] = averageCapsPerLine\n",
        "    xFeatures[i+9] = averageDigitsPerLine\n",
        "    xFeatures[i+10] = averageSpacesPerLine\n",
        "    xFeatures[i+11] = maximumOccurOfID\n",
        "    xFeatures[i+12] = averageBlockLength\n",
        "    xFeatures[i+13] = maxIndentDepth\n",
        "    \n",
        "    return xFeatures #Just the xFeatures are computed here and returned to caller. In data preparation for training, the caller will add the y label to train on, but in the live IDE the y label will be predicted by the readability features model.\n",
        "\n",
        "np.set_printoptions(suppress=True) #Turn off scientific notation\n",
        "datasetMatrix = SnippetsToFeatures() #get data out\n",
        "np.save(\"/content/drive/MyDrive/A Level/Computer Science/NEA/ReadabilityFeatures.npy\",datasetMatrix) #Save as numpy binary for later import into Python code\n",
        "print(\"Saved as NPY\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved as NPY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z-epsTJlb5k"
      },
      "source": [
        "### Train and Use Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGshyC5G3NSw"
      },
      "source": [
        "DATA_LOCATION =\"/content/drive/MyDrive/A Level/Computer Science/NEA/ReadabilityFeatures.npy\" #The location of the feature array for the readability dataset\n",
        "features = np.load(DATA_LOCATION)\n",
        "X = features[:-1,:]\n",
        "Y = features[-1,:].reshape((1,X.shape[1]))\n",
        "\n",
        "#Randomly re-order the dataset. (https://stackoverflow.com/questions/20546419/shuffle-columns-of-an-array-with-numpy) \n",
        "np.random.seed(1234)\n",
        "order = np.random.permutation(X.shape[1]) #Get a permutation of numbers which defines in what order we will put the dataset\n",
        "X = X[:, order] #Make sure to reorder both X and Y in the same way so that the labels still match their partner features.\n",
        "Y = Y[:, order]\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/A Level/Computer Science/NEA/ReadabilityX.npy\",X) #Save randomly shuffled data as numpy binary for later import into Python code\n",
        "np.save(\"/content/drive/MyDrive/A Level/Computer Science/NEA/ReadabilityY.npy\",Y) #This is important as it means we can separately run this part of the code to reshuffle the dataset but we don't have to run it when training, meaning we can easily compare hyperparameter values as the training set is constant."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHC_vjnJj-_4"
      },
      "source": [
        "def tanh(matrix):\n",
        "    \"\"\"The hyperbolic tangent activation function. Used as the default activation for all layers except the last one here\"\"\"\n",
        "    return (np.exp(matrix) - np.exp(-matrix))/(np.exp(matrix)+np.exp(-matrix))\n",
        "\n",
        "def tanh_prime(matrix):\n",
        "    \"\"\"Derivative of the hyperbolic tangent activation function with respect to the input matrix\"\"\"\n",
        "    return 1-np.square(tanh(matrix))\n",
        "\n",
        "def relu(matrix):\n",
        "    \"Rectified linear units activation function\"\n",
        "    return np.maximum(matrix,np.zeros(matrix.shape))\n",
        "\n",
        "def relu_prime(matrix): \n",
        "    \"\"\"Derivative of ReLU with respect to matrix\"\"\"\n",
        "    return np.vectorize(lambda z : 1 if z >=0 else 0)(matrix)\n",
        "\n",
        "def sigmoid(matrix):\n",
        "    \"\"\"Sigmoid activation function\"\"\"\n",
        "    return 1/(1+np.exp(-matrix))\n",
        "\n",
        "def sigmoid_prime(matrix):\n",
        "    \"\"\"Sigmoid activation function derivative\"\"\"\n",
        "    return sigmoid(matrix)*(1-sigmoid(matrix))\n",
        "\n",
        "def forwardProp(X,Ws,bs): \n",
        "    \"\"\"Performs 1 forward propagation through the network to get a readability rating for given X (input features - each column is one training example so vectorised), Ws (list of weights), bs (list of biases)\"\"\"\n",
        "    As = [X] #Activations at each layer, with layer 0 having the input features as its activations\n",
        "    Zs = [] #Z values (input to the activation function === matrix product of Ws[current layer] and a[layer before] + bs[current layer])\n",
        "    \n",
        "    for i in range(len(Ws)-1): #For every layer in the network except the last one, which is dealt with separately as it has a different activation,\n",
        "        Zs.append(np.matmul(Ws[i],As[i]) + bs[i]) #Compute the next Z value by multiplying the corresponding weights with the previous layer's activations and adding the biases\n",
        "        As.append(sigmoid(Zs[-1])) #Compute this layer's activation by applying the tanh activation function\n",
        "\n",
        "    #Forward prop the final layer\n",
        "    Z_final = np.matmul(Ws[-1],As[-1]) + bs[-1] #Compute Z as usual \n",
        "    Zs.append(Z_final) #Save the final Z value\n",
        "    As.append(Z_final) #No final activation function as we just want to do regression.\n",
        "    return Zs,As #Return the Z and Activation values, which will be needed for backprop \n",
        "\n",
        "def getCodeReadabilityScore(featureMatrix,Ws,bs):\n",
        "    \"\"\"Runs forward propagation on the network with the provided weights and input features and returns the final layer activations, the scores for each code sample\"\"\"\n",
        "    Zs, As = forwardProp(featureMatrix,Ws,bs)\n",
        "    return As[-1] #===y_hat\n",
        "\n",
        "def accuracyMetrics(X,Ws,bs,Y):\n",
        "    \"\"\"Calculate some accuracy metrics on the test data to check how good the learning algorithm is.\"\"\"\n",
        "    Y_hat = getCodeReadabilityScore(X,Ws,bs)\n",
        "    Y_hat *= sigmaY\n",
        "    Y_hat += muY\n",
        "    Y *= sigmaY\n",
        "    Y += muY\n",
        "    print(\"Y\")\n",
        "    print(Y)\n",
        "    print(\"Y_hat\") \n",
        "    print(Y_hat)\n",
        "    print(\"Diffs\")\n",
        "    print(Y-Y_hat)\n",
        "    print(\"Mean diff\")\n",
        "    print(np.mean(np.abs(Y-Y_hat)))\n",
        "\n",
        "def computeCost(X,Ws,bs,Y,lamd):\n",
        "    \"\"\"Computes the neural network cost function for the parameters Ws (weights), bs (biases) on input X and ground truth labels Y. lamd is the regularisation parameter lambda\"\"\"\n",
        "    Zs,As = forwardProp(X,Ws,bs) #Perform forward propagation to get the Zs and As. We will need to pass these back to the calling code for backprop\n",
        "    Y_hat = As[-1] #Precicted scores \n",
        "    m = Y.shape[1] #Number of training examples (needed as we average the cost over all training examples)\n",
        "    J = 1/m * np.sum(np.square(Y-Y_hat)) + lamd/(2*m)*sum([np.sum(np.square(w)) for w in Ws]) #MSE for regression with Frobenius norm regularisation (results in a scalar value - the smaller the value the better the neural network is doing)\n",
        "    return J,Zs,As #Cost, Zs, Activations\n",
        "\n",
        "def train(X,Ws,bs,Y,iter,alpha,lamd):\n",
        "    \"\"\"Train a deep neural network with initial parameters Ws, bs, for iter iterations, with input features X and ground truth labels Y, with batch gradient descent (learning rate alpha) and weight decay regularisation parameter lamd\"\"\"\n",
        "    Js = [] #We will save the costs on each iteration to be able to plot them later to see that they are decreasing monotonically\n",
        "    CVJs = []\n",
        "    m = Y.shape[1] #Number of training examples\n",
        "\n",
        "    for i in range(iter): #For each iteration, perform forward prop, get cost, perform backprop by gradient descent and update weights\n",
        "        costCV,_,_ = computeCost(cvX,Ws,bs,cvY,lamd)\n",
        "        cost,Zs,As = computeCost(X,Ws,bs,Y,lamd)\n",
        "        CVJs.append(costCV)\n",
        "        Js.append(cost)\n",
        "\n",
        "        dZ = -2*(Y-As[-1]) #Derivative of the cost function with respect to the final layer logits Z \n",
        "        dWs = copy.deepcopy(Ws) #The derivatives of the cost function with respect to each layer's weights..\n",
        "        dbs = copy.deepcopy(bs) #...and biases. We use copy to initialise a dummy list of matrices with the correct dimensions although the values will be replaced - the W and dW values are not the same\n",
        "        \n",
        "        for backwardLayer in range(len(As)-2,-1,-1): #Move backward through the network, starting at the second to last set of activations\n",
        "            dWs[backwardLayer] = 1/m*np.matmul(dZ,As[backwardLayer].T) + lamd/m*Ws[backwardLayer] #Compute the partial derivative with respect to the cost function J of the weights at this layer\n",
        "            dbs[backwardLayer] = 1/m*np.sum(dZ,axis=1,keepdims=True) #Compute the partial derivative with respect to the cost function J of the biases at this layer\n",
        "            \n",
        "            if backwardLayer != 0: #If we are not back at the start of the network, then there is at least one more iteration (set of dW and db values to compute) and so we need to compute the parital derivative of J with respect to the next set of Z values, since this is used to compute the next layer's dW and db\n",
        "                dZ = np.matmul(Ws[backwardLayer].T,dZ)*sigmoid_prime(Zs[backwardLayer-1])\n",
        "\n",
        "        for i in range(len(Ws)): #Apply gradients via gradient descent to update the weights so that they hopefully get it right next time\n",
        "            Ws[i] = Ws[i] - alpha*dWs[i]\n",
        "            bs[i] = bs[i] - alpha*dbs[i]\n",
        "\n",
        "    plt.ylim(0, 6)\n",
        "    plt.plot(Js) #debug plot of the cost function with iterations to check it is monotonically increasing\n",
        "    plt.plot(CVJs)\n",
        "    plt.title(\"Neural Network Cost Function with iterations of gradient descent\")\n",
        "    return Ws,bs #Trained weights and biases go back to calling code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asTSj1WnjsT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc778cc-78dd-47c6-bea2-9459690ca448"
      },
      "source": [
        "X = np.load(\"/content/drive/MyDrive/A Level/Computer Science/NEA/ReadabilityX.npy\") #Load in the X and Y values from where they are stored\n",
        "Y = np.load(\"/content/drive/MyDrive/A Level/Computer Science/NEA/ReadabilityY.npy\")\n",
        "\n",
        "n_X = X.shape[0] #The number of input features.\n",
        "\n",
        "#Normalise features to have 0 mean and 1 standard deviation\n",
        "np.set_printoptions(suppress=True) #Turn off scientific notation\n",
        "mu = np.mean(X,axis=1).reshape(n_X,1)\n",
        "sigma = np.std(X,axis=1).reshape(n_X,1)\n",
        "for i in range(sigma.shape[0]):\n",
        "    if sigma[i,0] == 0:\n",
        "        sigma[i,0] = 1 #Some rows have 0 standard deviation which leads to division by zero error so just make the standard deviation 1 for now \n",
        "X -= mu\n",
        "X /= sigma\n",
        "\n",
        "print(Y)\n",
        "muY = np.mean(Y)\n",
        "sigmaY = np.std(Y)\n",
        "Y -= muY\n",
        "Y /= sigmaY\n",
        "print(Y)\n",
        "\n",
        "#Split the dataset into train, cross-validation and test sets\n",
        "PERCENTAGE_TRAIN = 0.7 \n",
        "PERCENTAGE_CV = 0.15\n",
        "NUMBER_TRAIN = round(PERCENTAGE_TRAIN * X.shape[1])\n",
        "NUMBER_CV = round(PERCENTAGE_CV * X.shape[1])\n",
        "\n",
        "trainX = X[:,:NUMBER_TRAIN]\n",
        "cvX = X[:,NUMBER_TRAIN:NUMBER_TRAIN+NUMBER_CV]\n",
        "testX = X[:,NUMBER_TRAIN+NUMBER_CV:]\n",
        "\n",
        "trainY = Y[:,:NUMBER_TRAIN]\n",
        "cvY = Y[:,NUMBER_TRAIN:NUMBER_TRAIN+NUMBER_CV]\n",
        "testY = Y[:,NUMBER_TRAIN+NUMBER_CV:]\n",
        "\n",
        "\n",
        "m = trainX.shape[1] #Number of examples in training set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.3989071  8.19736842 5.84975369 6.89473684 5.951417   6.62857143\n",
            "  5.95108696 5.83333333 4.65277778 5.00961538 6.28698225 6.80962343\n",
            "  7.35714286 7.78205128 8.01020408 7.32804233 7.08333333 5.90517241\n",
            "  4.08613445 4.93811881 6.27118644 5.40178571 6.98412698 6.69421488\n",
            "  7.8957529  6.73553719 6.812749   3.92011834 6.96351931 6.00896861\n",
            "  5.04237288 6.40167364 8.2486631  6.98979592 6.21010638 5.84661355\n",
            "  3.22972973 7.44877049 6.03293413 5.01004016 4.6        7.78074866\n",
            "  6.125      7.93859649 7.68918919 5.87837838 6.29781421 7.42521368\n",
            "  5.99710983 5.95430108 6.61392405 6.28140704 6.84357542 7.29700855\n",
            "  7.02185792 5.03232759 4.93421053 6.36740331 5.18055556 5.28074866\n",
            "  6.10406091 5.84355828 8.50980392 6.85555556 4.70588235 6.53846154\n",
            "  7.35232068 6.56593407 5.86419753 6.99152542 5.25900901 7.05020921\n",
            "  5.5995935  7.2361809  6.56578947 5.97826087 6.38554217 6.52663934\n",
            "  7.1657754  4.85294118 5.94444444 5.31791908 7.47126437 6.22994652\n",
            "  7.34304933 4.37739464 6.64473684 6.20454545 4.68112245 7.\n",
            "  5.87184874 7.87076271 7.93193717 7.43686869 7.63374486 5.78193833\n",
            "  7.04896907 5.57053942 5.87837838 5.87797619 6.15384615 7.23463687\n",
            "  7.90983607 6.55487805 8.01136364 6.58959538 3.54700855 7.74193548\n",
            "  6.09251969 6.04278075 4.81481481 6.73155738 7.90358744 6.52083333\n",
            "  5.1863354  6.79956897 5.63577586 7.04787234 7.18879668]]\n",
            "[[-1.81689311  1.72383223 -0.46449042  0.50958776 -0.36972525  0.2614824\n",
            "  -0.3700329  -0.47979662 -1.58024825 -1.24762305 -0.05692902  0.43024953\n",
            "   0.94061826  1.33669549  1.5493675   0.91349229  0.68538747 -0.41283202\n",
            "  -2.10844328 -1.31426839 -0.07165304 -0.88206255  0.59291254  0.32267176\n",
            "   1.44268214  0.36119024  0.43316302 -2.26319474  0.57370315 -0.31607867\n",
            "  -1.21708824  0.04998025  1.77164642  0.59819682 -0.12858865 -0.4674175\n",
            "  -2.9067386   1.02602871 -0.29373928 -1.2472271  -1.62944491  1.33548126\n",
            "  -0.20792033  1.48261868  1.25013435 -0.437808   -0.04683204  1.00407029\n",
            "  -0.32713281 -0.36703687  0.24782889 -0.06212594  0.46189778  0.88456423\n",
            "   0.62808333 -1.22645194 -1.31791148  0.01803526 -1.08828164 -0.99488691\n",
            "  -0.22743864 -0.47026546  2.01506812  0.47306503 -1.53074697  0.17748671\n",
            "   0.93612328  0.20309515 -0.45102664  0.59980897 -1.01515147  0.6545109\n",
            "  -0.69767657  0.82786391  0.20296037 -0.34470281  0.03494334  0.16646669\n",
            "   0.76223561 -1.39366649 -0.3762247  -0.96023861  1.04699632 -0.11009472\n",
            "   0.92748102 -1.83694589  0.27655094 -0.13377225 -1.55382684  0.60770853\n",
            "  -0.44389459  1.41938761  1.47641121  1.01493448  1.19845206 -0.52770433\n",
            "   0.65335491 -0.72475925 -0.437808   -0.4381829  -0.18103147  0.82642464\n",
            "   1.45580973  0.19278932  1.55044838  0.22515098 -2.61098802  1.29930166\n",
            "  -0.23819677 -0.28456078 -1.42920587  0.35748047  1.44998509  0.16105463\n",
            "  -1.08289397  0.42087729 -0.66394927  0.6523326   0.78369484]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DUdWqG7dR_44",
        "outputId": "d85073d1-8e85-4ff4-878e-4fe1b6516230"
      },
      "source": [
        "NODES = [n_X,10,3,1] #This describes the architecture of the network (number of nodes in each layer). The activation functions are set elsewhere\n",
        "NODES = [n_X,15,10,5,1] #This describes the architecture of the network (number of nodes in each layer). The activation functions are set elsewhere\n",
        "NODES = [n_X,30,20,20,10,1] #This describes the architecture of the network (number of nodes in each layer). The activation functions are set else  0.31694589023189906where 1.1532639907258033\n",
        "\n",
        "#Randomly initialise weights and biases for each layer\n",
        "Ws = [] #Each value in this array is a numpy matrix representing the weights for the transition from a pair of consecutive layers. len(Ws) === len(nodes)-1\n",
        "for i in range(len(NODES)-1):\n",
        "    Ws.append(np.random.standard_normal((NODES[i+1],NODES[i])))\n",
        "bs = []#Each value in this array is a numpy matrix representing the biases for the transition from a pair of consecutive layers. len(bs) === len(nodes)-1\n",
        "for i in range(len(NODES)-1):\n",
        "    bs.append(np.random.standard_normal((NODES[i+1],1)))\n",
        "\n",
        "#MAINTENANCE: Network training constants - to improve performance of model try changing these and check train and cv set accuracy. Use test set to get a final unbiased estimate of how good the model is\n",
        "NUM_ITER = 20000 #How many epochs to train for\n",
        "ALPHA = 0.01 #Gradient descent learning rate\n",
        "LAMBDA = 0 #Regularisation parameter \n",
        "\n",
        "trainedWs,trainedbs = train(trainX,Ws,bs,trainY,NUM_ITER,ALPHA,LAMBDA) #Train the network\n",
        "print(\"Trained\")\n",
        "print(\"\\nTrain set\")\n",
        "accuracyMetrics(trainX,trainedWs,trainedbs,trainY) #Compute accuracy metrics on the train set\n",
        "print(\"\\nCV set\")\n",
        "accuracyMetrics(cvX,trainedWs,trainedbs,cvY) #Compute accuracy metrics on the dev set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained\n",
            "\n",
            "Train set\n",
            "Y\n",
            "[[4.3989071  8.19736842 5.84975369 6.89473684 5.951417   6.62857143\n",
            "  5.95108696 5.83333333 4.65277778 5.00961538 6.28698225 6.80962343\n",
            "  7.35714286 7.78205128 8.01020408 7.32804233 7.08333333 5.90517241\n",
            "  4.08613445 4.93811881 6.27118644 5.40178571 6.98412698 6.69421488\n",
            "  7.8957529  6.73553719 6.812749   3.92011834 6.96351931 6.00896861\n",
            "  5.04237288 6.40167364 8.2486631  6.98979592 6.21010638 5.84661355\n",
            "  3.22972973 7.44877049 6.03293413 5.01004016 4.6        7.78074866\n",
            "  6.125      7.93859649 7.68918919 5.87837838 6.29781421 7.42521368\n",
            "  5.99710983 5.95430108 6.61392405 6.28140704 6.84357542 7.29700855\n",
            "  7.02185792 5.03232759 4.93421053 6.36740331 5.18055556 5.28074866\n",
            "  6.10406091 5.84355828 8.50980392 6.85555556 4.70588235 6.53846154\n",
            "  7.35232068 6.56593407 5.86419753 6.99152542 5.25900901 7.05020921\n",
            "  5.5995935  7.2361809  6.56578947 5.97826087 6.38554217 6.52663934\n",
            "  7.1657754  4.85294118 5.94444444 5.31791908 7.47126437]]\n",
            "Y_hat\n",
            "[[4.38383937 8.18421841 5.84920007 6.89664584 5.95266582 6.63282733\n",
            "  5.09449074 5.83431244 4.65240393 6.18321324 6.29779599 6.81090557\n",
            "  6.18321324 7.78275839 8.01488382 7.32805779 7.08488189 5.90542731\n",
            "  4.07610389 4.93660578 6.26954966 5.40091016 6.98512037 6.69351356\n",
            "  7.89626951 6.73563647 6.81122089 3.95351485 6.96515218 6.00895812\n",
            "  5.05372962 6.40187615 8.25240827 6.98887764 6.19931285 5.84516605\n",
            "  5.09449074 7.44909936 6.03281849 5.01127452 4.59828017 7.77930863\n",
            "  6.1231254  7.93666856 7.68925596 5.879384   6.29717099 7.42597832\n",
            "  5.99757299 5.95484155 6.61429002 6.28190731 6.84467861 7.29722601\n",
            "  7.02594283 5.03369366 4.93448153 6.3668478  5.18049016 5.28083965\n",
            "  5.09449074 5.84597548 8.50884733 6.85392614 4.70571803 6.53760224\n",
            "  7.35245154 6.56582211 5.86427134 6.99354355 5.25982743 7.04982167\n",
            "  5.59862213 7.23604418 6.56728366 5.94938638 6.38640539 6.52493281\n",
            "  7.16454786 4.85385929 5.94646382 5.31706264 7.47562371]]\n",
            "Diffs\n",
            "[[ 0.01506773  0.01315002  0.00055362 -0.001909   -0.00124882 -0.0042559\n",
            "   0.85659621 -0.0009791   0.00037385 -1.17359786 -0.01081374 -0.00128214\n",
            "   1.17392961 -0.0007071  -0.00467974 -0.00001546 -0.00154856 -0.0002549\n",
            "   0.01003056  0.00151303  0.00163678  0.00087555 -0.00099339  0.00070132\n",
            "  -0.00051662 -0.00009928  0.00152812 -0.0333965  -0.00163287  0.00001049\n",
            "  -0.01135674 -0.00020251 -0.00374517  0.00091828  0.01079353  0.0014475\n",
            "  -1.86476102 -0.00032887  0.00011564 -0.00123436  0.00171983  0.00144003\n",
            "   0.0018746   0.00192793 -0.00006677 -0.00100562  0.00064322 -0.00076465\n",
            "  -0.00046316 -0.00054048 -0.00036597 -0.00050027 -0.00110319 -0.00021746\n",
            "  -0.0040849  -0.00136607 -0.000271    0.00055552  0.00006539 -0.00009098\n",
            "   1.00957017 -0.0024172   0.00095659  0.00162942  0.00016433  0.0008593\n",
            "  -0.00013087  0.00011195 -0.00007381 -0.00201813 -0.00081843  0.00038753\n",
            "   0.00097137  0.00013672 -0.00149419  0.02887449 -0.00086322  0.00170654\n",
            "   0.00122754 -0.00091811 -0.00201937  0.00085643 -0.00435934]]\n",
            "Mean diff\n",
            "0.07578835634455909\n",
            "\n",
            "CV set\n",
            "Y\n",
            "[[6.22994652 7.34304933 4.37739464 6.64473684 6.20454545 4.68112245\n",
            "  7.         5.87184874 7.87076271 7.93193717 7.43686869 7.63374486\n",
            "  5.78193833 7.04896907 5.57053942 5.87837838 5.87797619 6.15384615]]\n",
            "Y_hat\n",
            "[[6.19950781 4.9205675  5.66024565 4.86832004 4.68548389 5.90144265\n",
            "  7.39327321 4.85405039 6.48779862 5.75464234 5.58383324 6.192194\n",
            "  5.33408724 5.43276469 5.68898093 6.10774287 4.14986126 7.39536599]]\n",
            "Diffs\n",
            "[[ 0.03043871  2.42248183 -1.28285102  1.7764168   1.51906157 -1.22032021\n",
            "  -0.39327321  1.01779835  1.38296409  2.17729484  1.85303545  1.44155086\n",
            "   0.44785109  1.61620439 -0.11844152 -0.22936449  1.72811493 -1.24151984]]\n",
            "Mean diff\n",
            "1.2166101769489803\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEICAYAAACXo2mmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwdVZn/8c9ze0mns3RI0mzZCZsQZTFssgyiyKKCgigjLgiYUXQGUUYddUZmGEdcRkBhRBSVfRMYUX8oYZAgIMEkBoGwZCGQhOxk37v7+f1xziWVm3t7y62uTuX7fr1u39punVNVp+qpOqeq2twdERGRaihknQEREckPBRUREakaBRUREakaBRUREakaBRUREakaBRUREamanS6omNnlZnZr1vnYUWY22szczGqzzktvZ2ZrzWyf3pq+mc01s3d3cl7nmdlD1ctd15nZ9Wb2r1nmIcnMDjCz6Wa2xsz+KYP0zzezxxP9mZU3M3vUzC7KIu1q6TCoxB1miZn1Swy7yMweTTVn3WBmJ8YD9f+UDH/czM7v5DzczPZNJYM7wMwGmtnVZvZaLPSzY//QHZjniWY2v4Npfmlmm2Oaxc9HuptmJ/K03U7l7v3dfU5aaXYkmX5cH/+5A/O6zd3fU+xPu7yVHjBjHj7j7leklWY3fBn4o7sPcPcfZp2ZapW3HS0rvUlXToI7e6VSA1yyY9nqWJXO2tcBHzez0VWYVyq6upxmVg/8H3AwcCowEDgGWA4cWfUMbu+7cUcrfu7qgTSlAzm6yh0FPF+NGeVoney83L3dDzAX+CrwBjAoDrsIeDQxzYHAxDjNS8CHE+MeBS5K9J8PPJ7od+BzwEzglTjsGmAesBqYChyfmP5y4NYKeT0RmA/8CPhFYvjjwPmJ/guAF4AVwB+AUXH4YzE/64C1wEeAScDZcfyxcfx7Y/+7gOmxuwB8A3gVWALcDDTFcaPj7y4EXovpFIfVxmnOjut6XJnlughYDPRvZzu9Ja7rlYQd9IzEuNOBGcAaYAFwGdAP2AC0xWVdC+xdZr6/BP6zo+HFdV9Sbi4D/gasAu4CGhLjzwSmx208mxAsvwW0Ahtjfq5NlJF9Y3dTXLdL47r+BlBIli3g+3HbvgKcVmF9fQr4TaJ/JnBPon8ecGgyfWACsAXYHPP3m84sa0m65xPLP2XKWxz+vrhuVgJPAm8rWa9fiWltAmoJ++fsuH1nAB9MlImNcZ2uBVZW2HafBmYR9t8HkuUg5u8zcf2sBK4DLI7bl7B/rAKWAXe1Uz7PIJTLlYRy+pY4/JGSbb5/md+OietqDfBwzMOtlfatOPweYFHM22PAwYn5DYnLuRp4GriC7Y9JxfLWh1CeXiPsg9cDfUuON18i7PMLgU/FcWXLSpllOxl4Mebz2rg+k8fLSscqA66K6a4GniUeO4C+wH8T9o9VhH2imOejCWVqJfAMcGLJsfoK4Im4rh8ChsZxr8X1UjxWHFNxW1caUVKI3w3cRyyIJIIK4eA0j7CT1gKHxQJ2UCKjHQWVicDgxIJ/LG742rjBFhF3UjoXVPaMK/qA0qBCOJjNIuxwtYSD0pPlClTs/w/gR7H7a4Sd9zuJcdckNv4sYB+gf1xft5QU/Jvj+uqbGFYb192sZLoly3UncFM726gu/v5rQD1wUiwUxeVfSAzMwG7A4eUCQYV5/5LuB5Wngb3jtn0B+EwcdyShsJ9MCMbDgAPLlZcyO/nNwK+BAXEdvgxcmChbWwgHyRrgs8DrxINgyTz3IexYhZjHV4v5j+NWsDVYJdPfbn20t6xl0j2fCgew2H8Y4UBxVFyGT8b590mkNR0Ywdb95ZyYdoFwIrQO2KtceqXLEMvKMuBwwgH0R8QDcyJ/vwUGASMJwfzUOO4O4Osx3QbguArLvH/M08mEsvplQnmtr7TNS37/Z8KBvR44jrBvlwaVN/etxP44IC7T1cSTv8T+dHecfhzhRKtSULmKEIAGx/n9Bvh2osy3EI4DdYSTt/XAbu3tO4l0hhL20w/F318a53dRR8cq4BTCCfcgQoB5S2KbXxfX6TBCGXpHXA/DCLUbp8dtdnLsb05sh9lxe/WN/VeWrOfa9o4X7l0LKuMIB4Jmtg0qHwH+VPKbnwDfLFdgKL9TndRBHlYAh8Tuy+kgqMTu7xLPnNg2qDxIPAjF/kIsCKMq7OTvAv4Wu38fl/2p2D8JOCt2/x9wceJ3BxAOcLWJDbJPYnxx2GWEs8vh7Sz/xOLGrTD+eELgLSSG3QFcHrtfA/4BGFhpfbUz718SziJXxs+ycjtM6bxiuflYov+7wPWJ8nFVhfS2KS/JbULYQTYTT1jiuH9IlMXzgVmJcY3xt3tWSGse4WB6LnADITAcSAjyD1Q4yGyz3B0ta5k0z6f9oPJj4IqS37wE/F0irQs62GbTgTPLpVe6DMCNhOrN4rj+hHI7OpG/4xLj7wa+GrtvjuutYtmN0/0rcHfJPreAeJZcbpsnph1JONA2JobdyvZBZZ920h8Up2mKZWgL8SQmjv+vctuEcLBeB4xNjDuGrTUqJxKu9msT45cAR1cqKyX5+gTxWBL7jXBSXAwqFY9VhJOBlwlXHoWSaTYQj5cl6X2FeKKbGPYH4JOJ7fCNxLiLgd+XrOcOg0qn7/5y9+cIZyxfLRk1CjjKzFYWP8B5hKuFzpqX7DGzy8zsBTNbFefXRIjqXfEd4BQzO6RMfq9J5PUNwsYcVmE+fwb2N7M9gEMJO9KI2EB+JOHSGrae7Ra9Sggoe1Razuifgevcvb0G8+XAXu2M3xuY5+5tJekXl+lswtnJq2Y2ycyOaWde5Xzf3QfFT1e2w6JE93rCAQvCWfbsLuYBQhmoY/v1nNx2b6bp7utjZ3/Km0Q4MJwQux8F/i5+JnUxb5WWtatGAV8q2Z9GELZxUen+8ol491Rx+nF0fn/Zpty6+1pCeSu7Ttl22b5M2HeeNrPnzeyCTqbRFpeh0j5X+ts3EtsSyu9Hbw4zsxozuzLezLKaEIghrJNmwn6ZnEeyPCU1E05MpibW7e/j8KLl7t6S6O/Ktt87mQ8PR+9kvioeq9z9EUJ12XXAEjO7wcwGxmVsoPz+NQo4p6RsHce2x5YdLsddvaX4m4SqhWRhmAdMShx0BnlozP1sHL+OsGGKygUbL3aY2fGEwvphwmXkIMIVknUlo+6+nHDZW3qXyzzgH0ry29fdn6wwn/WEy8xLgOfcfTOhTvKLwGx3XxYnfZ2w0YqKZ1iLyy1nwnuAb5jZ2e0szsOEANmvwvjXCYEuuT1HEs4Gcfe/uPuZwO7A/xLONivlp7M6s10rmQeMrTCuvTwtI5xllq7nBV1IO6kYVI6P3ZPoOKjsyDrrjHnAt0rKZ6O731EuD2Y2Cvgp8HlgSNxfnmPr/tJRfrcpt7GMDaET69TdF7n7p919b8IV4/9UuJOtNA0jBMrObLeFwGAzS5a1EeWyk+j+KKHq6N2EE9LRxaQJ1XctJfMYWSHtZYSz/oMT26LJ3Tt7oO1o3S9M5iOxXoraPVa5+w/d/e3AQYQqq3+Oed5I+f1rHuFKJTm/fu5+ZRWW5U1dCiruPovQCJm8l/y3hDP5j5tZXfwcYWZvieOnA2eZWWMscBd2kMwAwkZfCtSa2b8R7nbqjh8Q6hPfkhh2PfAvZnYwgJk1mdk5ifGLCXXqSZMIO23xQPNoST+E6qZLzWyMmfUnXFLfVXIWU87zhEbq68zsjArT3EIoEPea2YFmVjCzIWb2NTM7HZhMOKv4clz/JwLvB+40s3oLz0Y0ufsWQn108YpmMTDEzJo6yGM504HTzWywme0JfKELv70R+JSZvSsuyzAzOzCRp7LPCLh7KyEgfsvMBsQD6hcJ1SHdMQl4J6Eefj7wJ8K2GAL8tcJvKuavm0rn91PgM2Z2lAX9zOy9Zjagwu/7EXb4pQBm9inClUpy/sPjHYTl3EHYFoeaWR9CuZ3s7nM7yriZnWNmw2PvipiPtjKT3g28N27vOkI76SbCyVm73P1VYApweSzLxxDKdnsGxPkvJ5z4/Fdifq2E9s7L4zHpIEK7Vbm02wjb4yoz2z0u8zAzO6WjfEcdlZXfAQeb2VnxrrV/YtuTs4rHqniMPSquz3WEQNIW8/xz4Admtne8ajsmbttbgfeb2SlxeIOFxwqG07GlhG3bYdnvzsOP/0EoyAC4+xrC2fa5hDOSRYSqpz5xkqsI9eCLgZuA2zqY/x8Il5gvEy5LN1L+crdD7r6aUL89ODHs/pi/O+Ol8XPAaYmfXQ7cFC8PPxyHTSIU1Mcq9EPYkLfEYa/EfP9jJ/P5DOGOn5+a2Wllxm8inHW9SGhfKd61MpRwANhM2NFOI5yp/A/wCXd/Mc7i48DcuLyfIVRPEsffAcyJy5usYunILYS7R+YS7hLp9G3G7v40od3iKsJV6CS2nsleA3zIzFaYWblnFv6RsBPNIbSV3U5Y913m7i8T7mT5U+xfHef7RDz4lHMjcFBcX//bnXRLXE6ivLn7FEJtwLWEA/UsQrtIpWWYQbjT58+EfeythLt3ih4hnLgsMrNlZX7/MKHN417CmfNYwr7cGUcAk81sLaEx+xIv83yHu79EuPnmR4Ty+X7g/bHcdsZ5bL2F/j8JZW1TO9PfTDh2LCC0Vz5VMv7zhGqdRYR2j1+0M6+vELbBU3H/eZjQXtoZ7ZaVWMtxDnAlYdn2I7HtOjhWDSQEvBVxWZcD34vjLiPcDfYXQpXZdwjtLvMIV3BfIwSJeYSrmw7jQKyx+RbwRFyeoytNW7w1UERkp2BmdwEvuvs3s86LbG+ne02LiOxaYlXP2FhVeirhbLsaV4mSgtSDipkNMrNfmdmLFu7o6uqdRyKya9uT0I65Fvgh8Fl3r9TmJRlLvfrLzG4iPMfys9hY2OjuK1NNVEREMpFqUIl3FU0nPJikxhsRkZxL++VrYwh3GfzCwkOIUwl3iKxLTmRmEwjvyqFfv35vP/DAA7ebUUe2vP4cW2oaadwjszeki4hkYurUqcvcvbnjKdOX9pXKeMLtfMe6+2QzuwZY7e4V/5fD+PHjfcqUKV1Oa8G/78+igYfw9kvv6X6GRUR2QmY21d3HZ50PSL+hfj7hfVCTY/+vCO9aEhGRHEo1qLj7ImCemRUfFnoX4WGktFJMb9YiItKhnviHNv8I3Bbv/JpDeJK66rxrrwYTEZEUpB5U3H060Cvq+kREJF16ol5ERKomX0FFj8KIiGQqN0FFbSoiItnLTVAREZHsKaiIiEjV5CqomJ5TERHJVK6CioiIZEtBRUREqiZnQUXVXyIiWcpNUNEtxSIi2ctNUBERkewpqIiISNUoqIiISNXkJqg4pnd/iYhkLDdBRUREsqegIiIiVZOroKKbikVEspWjoKI3f4mIZC1HQUVERLKWq6CiaxURkWzlJqgonIiIZC83QUVERLKnoCIiIlWTr6CiJ+pFRDKVn6BiekpFRCRr+QkqIiKSOQUVERGpmtq0EzCzucAaoBVocffx6aWmNhURkSylHlSid7r7sjQT0L8TFhHJnqq/RESkanoiqDjwkJlNNbMJPZCeiIhkpCeqv45z9wVmtjsw0cxedPfHkhPEYDMBYOTIkd1OSO/+EhHJVupXKu6+IH4vAe4HjiwzzQ3uPt7dxzc3N3cvHYUUEZHMpRpUzKyfmQ0odgPvAZ5LM00REclO2tVfewD3W3javRa43d1/n1Ziuv9LRCRbqQYVd58DHJJmGiIi0nvk65ZiNaqIiGQqX0FFREQylbOgoksVEZEs5Sao6DUtIiLZy01QCXSlIiKSpZwFFRERyVKugooqwEREspWjoKKQIiKStRwFFbWoiIhkLVdBRa+UFBHJVm6CisKJiEj2chNUAEUWEZGM5SioqKFeRCRrOQoqoEsVEZFs5SaoKJyIiGQvN0FFRESyl5ugYmpSERHJXG6CCug5FRGRrOUmqOjV9yIi2ctNUAE11ouIZC1XQUXVXyIi2cpNUFH1l4hI9nITVEREJHsKKiIiUjX5CipqUhERyVSOgoraVEREspajoCIiIllTUBERkarpkaBiZjVm9lcz+22q6ahRRUQkUz11pXIJ8EKaCSiciIhkL/WgYmbDgfcCP0s7LRERyVZPXKlcDXwZaKs0gZlNMLMpZjZl6dKlPZAlERFJQ6pBxczeByxx96ntTefuN7j7eHcf39zcvAMpqhJMRCRLaV+pHAucYWZzgTuBk8zs1nSS0nMqIiJZSzWouPu/uPtwdx8NnAs84u4fSzNNERHJTr6eU3FVf4mIZKm2pxJy90eBR1Obv/5JvYhI5vJ1pSIiIplSUBERkarJVVDRa1pERLKVo6CiNhURkazlKKiIiEjWFFRERKRqchZU1KYiIpKl3AQVhRMRkezlJqiIiEj2FFRERKRqchRUdEuxiEjWchRUREQkawoqIiJSNbkKKqZX34uIZCo3QcXVpiIikrncBBUREcmegoqIiFRNzoKK2lRERLKUo6CiNhURkazlKKiIiEjWFFRERKRqchVU9O+ERUSylZug4mpSERHJXG6CioiIZE9BRUREqiZHQUX1XyIiWctRUBERkaylGlTMrMHMnjazZ8zseTP79zTTExGRbNWmPP9NwEnuvtbM6oDHzexBd38qjcT06nsRkWylGlTc3YG1sbcuflI58uvV9yIi2Uu9TcXMasxsOrAEmOjuk8tMM8HMppjZlKVLl6adJRERSUnqQcXdW939UGA4cKSZjSszzQ3uPt7dxzc3N6edJRERSUmP3f3l7iuBPwKnpphKerMWEZEOpX33V7OZDYrdfYGTgRdTSi2d2YqISKelfffXXsBNZlZDCGB3u/tvU05TREQykvbdX38DDkszjSS9pVhEJFu5eaLeFVJERDKXm6BialIREclcboKKiIhkL1dBRRVgIiLZyk1QUTgREcleboKKnlMREclejoKKiIhkTUFFRESqJjdBxRN/RUQkG7kJKiIikr1cBRXThYqISKZyFFR095eISNZyFFRERCRrCioiIlI1OQsqalQREclSboKKq01FRCRzuQkqevW9iEj2chNUREQke7kKKnr1vYhItnITVBRORESyl5ugoocfRUSyl6OgIiIiWVNQERGRqslZUFHLiohIlnIWVEREJEu5CipqqhcRyVZugope0yIikr1Ug4qZjTCzP5rZDDN73swuSTM9NamIiGSrNuX5twBfcvdpZjYAmGpmE919RrUT0nWKiEj2Ur1ScfeF7j4tdq8BXgCGpZhierMWEZEO9VibipmNBg4DJpcZN8HMppjZlKVLl3Zr/q7XFIuIZK5HgoqZ9QfuBb7g7qtLx7v7De4+3t3HNzc390SWREQkBakHFTOrIwSU29z9vrTTExGR7KR995cBNwIvuPsP0kwLTK++FxHJWNpXKscCHwdOMrPp8XN6Ggm1KaiIiGQu1VuK3f1xeuhuX6eA0dYTSYmISAX5eaLejIKuVEREMpWfoIJhrisVEZEs5Suo6EpFRCRTOQoqBQUVEZGMpf3ur55jCiqyE3GHtlZoawFvBW8L/d5W0l1pXGd/07o1reJ03gZ4GF76XXZYe9Mnx5EY19aNeZUMS66rrT3tD+/KtJ0eToXh1Zh/Yljf3eCkb5RPdyeSm6DipjYVqaB1C2xeC5vXbf20bIKWjdt+t25K9CfHbd52mtYtIRgUv9/s3gKtsb9tS/vTtbVkvVZ2MombSLd5JVO54V2ZNq3hVBheZvrisAF7K6j0LrpSyR132LQGNq6Ejatgw8rQXfq9cRVsWrt94Ni8Frash9bN3cyAQW0D1PbZ+l1TDzV1UKiN37G7vjF019RBoSbRXQc1tWGaN7vrts6jUANWE78LodsKUEh2J8YVasJBaJtxXfmNheFYPJhZ5WEQ52FlxlnlcRZr1bs0fekwvctvZ5WboOKm51R2GpvXw+oFsPp1WLsE1i2J30vDJ9ndXkCwAjQ0QcMg6DMA6vtD4xAYNCJ01/dLfPpDXePW/tqG+KlPdCeDR59w4NfBTaRLchRUjEKlelDpWRtWwPLZsPJVWDUfVi2I3/NCMFm/fPvfFOqgXzP0b4Z+u8MeB4f+xiHQd1AIHNt8N0H9gHB2LiK9Rm6CimG6UulJrVtgxVxYNhOWz4zfs8L3+mXbTtunCZqGQdNwGD4eBg6DphEwcG/ov3sIHn1301WBSA7kJqi4FdA/6aoy91AFVS5wrJgb7iwq6tcMQ/aDA08P30P3g91GhwDSMDCrJRCRHpaboIIV9JqW7tqyIVRXLZ8Jy2aFwFHs3rRq63S1DTB4LOw5Dg7+wNbgMWTfUCUlIru83AQV13Mq7WttgVWvwfI5IWi8MTtedcwKbR3JdTdwWAgUbzsnBo59w3fTCLVhiEi7chNU0Lu/QnXV6te3Bozls8Pnjdnwxivh+Yii+gEwZCyMPAqGfCx0F6866vtltwwislPLT1DprVcq7qFRe8u6UM20ZUN8EK7MU9FWCLfGFm+PrWvc9srAPTx7sfp1WDkvXGGsfC0GkdnwxpzwXEZRbQMM3geaD4ADTg8BY8jY8N2vWQ3jIlJ1uQkqbgUKWdz9tWktvD4Nlr4UPqvmx2cslsD6FeEgn2zQ7qqaPlDXEGqnNq1mu5sRCrUwaFQIFGNO2Bo0Bo8N1ViqrhKRHpSboNKjVyrLZ8Oz98Csh2HBtK1Bo34A7DYq3gk1FvoODlVJdX3DVUfxu1BT8iR1fPrZW0OQ2rQ6PEm+ZQO0bIAtG8NVRfEqZsBeoX1j0Ajov2d4SltEpBfIzdHI0v4nXW1t8NLv4IkfwvynAYPhR8Bxl8KoY2D3g8LBXlVKIrILy01QcVJ8TuWVx+DBr8CSGaGq6eQr4K0fCg/viYjIm3ITVFJ5TmXLRvjdF2H6bSGYnH0jHPQBVTeJiFSQn6OjVfk1LWuXwJ3nhaqu4y+DEy4LbSIiIlJRjoJKFa9UFs+A2z8S7uL68M1w0JnVma+ISM7lK6hU4+HHmQ/DPeeH/4/xqf8Hww7f8XmKiOwi8vMQQzWeU3n6p3D7OeFFiJ9+RAFFRKSLcnOl0laoo8Y8vOOqqw3prS3wh6/B0z+B/U8NDfJ9+qeTURGRHMtNUGmpiY3oW9ZDTRdetb5xNfzqApg1EY7+HLznivBAooiIdFlugkpbbWP43ryeQmf/f8fy2XDH34eXL77vKhh/QYo5FBHJv1TbVMzs52a2xMyeSzMdgNqGEFTWrV3duR+88Bv46TvDHV6f+LUCiohIFaTdUP9L4NSU0wCgru8AoBNBZdUCuPfTcNfHQoP8hD/CmOPTz6CIyC4g1eovd3/MzEanmUbRgCHhlSmPPj2NM0YfSmN9YtHa2mDBVHjmdvjrbeE183/31fBAY01dT2RPRGSXkJs2lYMPP5bNE/tw/MwrefTbDzBs92b2G1JL44ZF8Poz4d/i1vaFt30YTvjn8DZhERGpql4RVMxsAjABYOTIkd2aR03DAGrOu53+j1zNEYtnYoueYeWiOhY17E7fEaeyx7h3UjjwvdDZRnwREemyXhFU3P0G4AaA8ePHd/9dK/u+m6Z93w3Aa8vXc/vTr3H3lHm88dxmxizqx3lrlvGhtzcwqLG+KvkWEZFtmXu6/9gqtqn81t3HdWb68ePH+5QpU6qW/qaWVh58dhG3PPUqU19dQb/6Gi48bgwXnbAPAxvUniIiOz8zm+ru47POB6QcVMzsDuBEYCiwGPimu9/Y3m+qHVSSZry+muv+OIvfPbuQpr51XHbKAZx35EgKBf1jLRHZee0yQaU70gwqRc8tWMW3H3yBJ2Yt57CRg/j2WW/lwD3V1iIiO6feFFTy80LJLhg3rIlbLzyKqz5yCK8uX88Z1z7BLX+eS28LsCIiO5tdMqhA+J/2HzxsOBMvPYF3jB3Cv/76eS6+bRprN7VknTURkZ3WLhtUiob078PPP3kEXzv9QB6asZgP/fhJFqzckHW2RER2Srt8UAEoFIwJJ4zlF+cfwYIVGzjz2ieYPm9l1tkSEdnpKKgknLB/M/dd/A761hf4yE/+zK+nL8g6SyIiOxUFlRL77TGA/734WA4ZMYhL7pzOlQ++SGubGvBFRDpDQaWMIf37cOuFR/HRo0Zy/aTZfPrmKaxYtznrbImI9HoKKhXU1xb4rw++lSs+MI4/zVzKadf8iSdnLcs6WyIivZqCSgc+fvQo7r/4WBrrazjvxsl8+8EX2LilNetsiYj0SgoqnTBuWBO//afjOPeIEfxk0hxOvfoxXbWIiJShoNJJjfW1fPust3HrhUfhwEd/Npkv3f0MS9ZszDprIiK9hoJKFx2331D+8IUTuPjEsfx6+gLe+b1HufaRmaoSExFBQaVbGupq+PKpB/LQpSdw7L5D+f5DL/PO7z/KvVPn6/ZjEdmlKajsgH2a+3PDJ8Zz54SjGdK/ni/d8wwn/2AS9/91Pi2tbVlnT0SkxymoVMHR+wzhgc8dx4/PO5z62gKX3vUMJ1/1GL+aOp/NLQouIrLr2CX/n0qa2tqch2Ys4uqHZ/LiojU0D+jDx48exUePGsnQ/n2yzp6I5FBv+n8qCiopaWtzHpu5lF88MZdJLy+lvrbA+962F+ceMZIjRu+Gmf7bpIhUR28KKrVZZyCvCgXjxAN258QDdmfWkrXc9ORc7v/rAu6btoDRQxo5Z/wIzj58OHs2NWSdVRGRqtGVSg9av7mFB59dxN1T5jH5lTcoWHgz8tmHD+fkg/agoa4m6yyKyE6oN12pKKhkZO6ydfxq6nzunTafhas2MqBPLae9dU/OOnw4R44eTKGg6jER6RwFlXbsKkGlqLXNeWrOcu6btoDfP7eQdZtbGTaoLx88bBgfPHwYY5v7Z51FEenlFFTasasFlaT1m1uYOGMx901bwJ9mLqXN4ZDhTXzwsGG8/5C9GaK7x0SkDAWVduzKQSVpyeqNPPDM69w3bQEzFq6mtmAcu+9Q3jF2CEeOGcy4YU3U1egxIxFRUGmXgsr2Xly0mvunLWDiC4uZs3QdAI31NRw6YhDjhjVx8N4DGTesiTFD+qktRmQXpKDSDgWV9i1ds4m/zH2DyXOWM+21lby0aA2b4ythGutrGDO0H6OH9mP0kEZGD+nHyMGN7NnUwDlnHvsAAAf8SURBVO4DGuhbr7vLRPJIQaUdCipds6W1jZmL1/L866t4/vXVvLJsHXOXr2P+ig3bvdxyYEMtewxsYI+BDQzuV8+gxjoG9a2jqbE+fPetY1Bj+O7Xp5bG+hoa62upr1U1m0hv1puCih5+3MnV1RQ4aO+BHLT3QM5JDN/S2sb8FRuY98Z6lqzZxOLVG1myeiOLV29i8ZqNzF+xnpUbtrBqwxY6Oq+oqzEa64tBpubNgNNQV0N9TYH62sLW70R3XfzuE4fXFgrUFKBgRk0h8TGjUDBqC+G7pmR8cfragmEGhlEohO/QD+EFBcl+o2BbpyFOY2ZvTm+EabDSeW07jRW2HV6q3MsRyk9ZftrOTldunuWnKze/Mr/tStp6A4R0koJKTtXVFBgztB9jhvZrd7q2NmfNxhZWbtjMyvVb3gw06ze1sH5zK+s3t7Buc2uiv5V1m0P3G+s2s7mlLXxaS75b2mjRvwHY5XQ2yPUGvS1Q7tXUwONfOSnrbOyw1IOKmZ0KXAPUAD9z9yvTTlM6r1AwmhrraGqsY9SQ6s67rc1DkImBpqXVaXWnrc1pbXNa2pw2D92tsbulbev41pJxrW3EKj3HHdocPHY7UKzK9cTwNg/DHWC76UN/W+x5c1iyO863OG2pcld5lUJp+Wk7N8/y8+t+fiqlsUPLWC4/5ZPJXC+r9QdgQEM+zvFTXQozqwGuA04G5gN/MbMH3H1GmulK71AoGA2FGr1+RmQXknYL7JHALHef4+6bgTuBM1NOU0REMpL29dYwYF6ifz5wVOlEZjYBmBB715rZS91MbyiwrJu/TZPy1TXKV9coX12Tx3yNqmZGdkSvqMRz9xuAG3Z0PmY2pbfcVpekfHWN8tU1ylfXKF/pSrv6awEwItE/PA4TEZEcSjuo/AXYz8zGmFk9cC7wQMppiohIRlKt/nL3FjP7PPAHwi3FP3f351NMcoer0FKifHWN8tU1ylfXKF8p6nWvaRERkZ2XXuokIiJVo6AiIiJVk4ugYmanmtlLZjbLzL7aA+mNMLM/mtkMM3vezC6Jwy83swVmNj1+Tk/85l9i/l4ys1PSyruZzTWzZ2P6U+KwwWY20cxmxu/d4nAzsx/GtP9mZocn5vPJOP1MM/vkDubpgMQ6mW5mq83sC1msLzP7uZktMbPnEsOqtn7M7O1x/c+Kv+3UC6Yq5Ot7ZvZiTPt+MxsUh482sw2J9XZ9R+lXWsZu5qtq283CTTyT4/C7LNzQ09183ZXI01wzm57B+qp0bMi8jPWY8F6jnfdDuAFgNrAPUA88AxyUcpp7AYfH7gHAy8BBwOXAZWWmPyjmqw8wJua3Jo28A3OBoSXDvgt8NXZ/FfhO7D4deJDwzr+jgclx+GBgTvzeLXbvVsXttYjwsFaPry/gBOBw4Lk01g/wdJzW4m9P24F8vQeojd3fSeRrdHK6kvmUTb/SMnYzX1XbbsDdwLmx+3rgs93NV8n4/wb+LYP1VenYkHkZ66lPHq5UevxVMO6+0N2nxe41wAuEtwdUciZwp7tvcvdXgFkx3z2V9zOBm2L3TcAHEsNv9uApYJCZ7QWcAkx09zfcfQUwETi1Snl5FzDb3V/tIL+prC93fwx4o0x6O7x+4riB7v6Uh73/5sS8upwvd3/I3Vti71OE57wq6iD9SsvY5Xy1o0vbLZ5hnwT8qpr5ivP9MHBHe/NIaX1VOjZkXsZ6Sh6CSrlXwbR3gK8qMxsNHAZMjoM+Hy9jf564ZK6UxzTy7sBDZjbVwutvAPZw94WxexGwRwb5KjqXbXf2rNcXVG/9DIvd1c4fwAWEs9KiMWb2VzObZGbHJ/JbKf1Ky9hd1dhuQ4CVicBZrfV1PLDY3WcmhvX4+io5NuwMZawq8hBUMmNm/YF7gS+4+2rgx8BY4FBgIeESvKcd5+6HA6cBnzOzE5Ij49lNJveRx/ryM4B74qDesL62keX6qcTMvg60ALfFQQuBke5+GPBF4HYzG9jZ+VVhGXvddivx92x74tLj66vMsWGH5rczyUNQyeRVMGZWRyg0t7n7fQDuvtjdW929Dfgp4bK/vTxWPe/uviB+LwHuj3lYHC+bi5f8S3o6X9FpwDR3XxzzmPn6iqq1fhawbRXVDufPzM4H3gecFw9GxOql5bF7KqG9Yv8O0q+0jF1Wxe22nFDdU1syvNvivM4C7krkt0fXV7ljQzvzy7yMVV3ajTZpfwhvBZhDaBgsNgIenHKaRqjLvLpk+F6J7ksJ9csAB7NtA+YcQuNlVfMO9AMGJLqfJLSFfI9tGwm/G7vfy7aNhE/H4YOBVwgNhLvF7sFVWG93Ap/Ken1R0nBbzfXD9o2op+9Avk4FZgDNJdM1AzWxex/CQaXd9CstYzfzVbXtRrhqTTbUX9zdfCXW2aSs1heVjw29ooz1xCfzDFRlIcIdFC8TzkC+3gPpHUe4fP0bMD1+TgduAZ6Nwx8o2fm+HvP3Eom7NaqZ97jDPBM/zxfnR6i7/j9gJvBwonAa4Z+ozY75Hp+Y1wWEhtZZJALBDuStH+HMtCkxrMfXF6FaZCGwhVAffWE11w8wHngu/uZa4lsrupmvWYR69WIZuz5Oe3bcvtOBacD7O0q/0jJ2M19V226xzD4dl/UeoE938xWH/xL4TMm0Pbm+Kh0bMi9jPfXRa1pERKRq8tCmIiIivYSCioiIVI2CioiIVI2CioiIVI2CioiIVI2CioiIVI2CioiIVM3/B6ScKHot2rusAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTA3ewdUPjYp",
        "outputId": "5abc9e28-75c1-4d9a-e462-aa2e8ee3f268"
      },
      "source": [
        "np.save(\"/content/drive/MyDrive/A Level/Computer Science/NEA/ReadabilityWs.npy\",trainedWs) #Save weights as numpy binary for later import into Python code\n",
        "np.save(\"/content/drive/MyDrive/A Level/Computer Science/NEA/ReadabilityBs.npy\",trainedbs) #Save biases as numpy binary for later import into Python code"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjgiRy5jRDy8",
        "outputId": "d7aa8418-fcc3-4f96-94bb-20c80ef7fd9f"
      },
      "source": [
        "#Example code for loading in ws and bs to be used in the actual IDE code\n",
        "w = np.load(\"/content/drive/MyDrive/A Level/Computer Science/NEA/ReadabilityWs.npy\",allow_pickle=True)\n",
        "b = np.load(\"/content/drive/MyDrive/A Level/Computer Science/NEA/ReadabilityBs.npy\",allow_pickle=True)\n",
        "accuracyMetrics(testX,w,b,testY)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y\n",
            "[[7.23463687 7.90983607 6.55487805 8.01136364 6.58959538 3.54700855\n",
            "  7.74193548 6.09251969 6.04278075 4.81481481 6.73155738 7.90358744\n",
            "  6.52083333 5.1863354  6.79956897 5.63577586 7.04787234 7.18879668]]\n",
            "Y_hat\n",
            "[[7.76842178 7.20180623 5.60895306 7.75304005 6.81854477 5.87824022\n",
            "  6.99799124 4.35033828 6.68782782 7.64709433 5.93491358 7.75493712\n",
            "  3.80533604 6.66220338 4.13719479 6.75220119 7.15221704 5.80009983]]\n",
            "Diffs\n",
            "[[-0.53378491  0.70802983  0.94592499  0.25832359 -0.22894939 -2.33123168\n",
            "   0.74394425  1.74218141 -0.64504708 -2.83227951  0.7966438   0.14865032\n",
            "   2.71549729 -1.47586798  2.66237418 -1.11642532 -0.1043447   1.38869686]]\n",
            "Mean diff\n",
            "1.1876776157512687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_hnHPnlPmF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07fbbeb-62c9-4eaa-c163-595066da23f4"
      },
      "source": [
        "#Module test\n",
        "\n",
        "codeToScore = \"\"\"def sorter(loI):\n",
        " done = [loI[0]] #Will store the sorted list################################\n",
        " for item in loI[1:]: #First one already sorted so skip\n",
        "   for i in range(len(done)):#Try to insert next one where it belongs\n",
        "     if done[i] > item: #Correct posn\n",
        "       done.insert(i,item) #Put it in the done list\n",
        "       break #No need to continue\n",
        "   else:\n",
        "     done.append(item) #Goes at end\n",
        " return done\"\"\"\n",
        "\n",
        "print(getCodeReadabilityScore(ExtractCodeReadabilityFeatures(codeToScore),trainedWs,trainedbs)*sigmaY+muY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7.38866033]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE3i0d25q7Kk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}