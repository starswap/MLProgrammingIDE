{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook: G4G Resources.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LenvVgkhk0tB"
      },
      "source": [
        "## **Geeks4Geeks Scrape and Process Resources with Neural Network Algorithm for Level Estimation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5DHVYAxbfZB"
      },
      "source": [
        "import numpy as np #Linear algebra library\n",
        "import requests #Used to make http requests to grab g4g pages\n",
        "import bs4 #Beautiful soup - html parser library\n",
        "import time #Used to time.sleep to prevent making too many requests to g4g\n",
        "import os #Used to manipulate local file system to store the scraped data\n",
        "import copy #Used for copy.deepcopy to copy matrices to get new ones with the same dimensions \n",
        "import matplotlib.pyplot as plt #Used to plot the cost function against time for the network."
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ZaRFqspCMS"
      },
      "source": [
        "### Scrape and Process Resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIvO9_RYDijR",
        "outputId": "2dc6e45d-aae4-44b5-d4c8-b2b8ce414e86"
      },
      "source": [
        "def ScrapeG4GResources():\n",
        "    \"\"\"Collects articles from Geeks for Geeks articles by web scraping and saves them to the local machine for later processing by the companion subroutine\"\"\"\n",
        "    SAFETY_DELAY = 3 #in seconds, to avoid accidentally DoSing the website which would not be a good outcome.\n",
        "    STORAGE_PATH = \"/content/drive/MyDrive/A Level/Computer Science/NEA/G4G Downloads\"\n",
        "\n",
        "    soup = bs4.BeautifulSoup(requests.get(\"https://www.geeksforgeeks.org/python-programming-language/\").text) #Create a beautiful soup object which will allow us to parse the html of the webpage (screenscrape it)\n",
        "    articlesLeftPanel = soup.find(\"article\",{\"id\":\"post-136942\"}).findChild().find_all(\"div\",recursive=False)[3] #Based on the structure of the G4G Python homepage, we know that the Python articles are found in two panels which are uniquely identified as the 4th and 5th divs inside an article tag with the class post-136942\n",
        "    articlesRightPanel = soup.find(\"article\",{\"id\":\"post-136942\"}).findChild().find_all(\"div\",recursive=False)[4]\n",
        "    aTags = articlesLeftPanel.find_all(\"a\",href=True) + articlesRightPanel.find_all(\"a\",href=True) #All these links point to other Python tutorial pages\n",
        "\n",
        "    print(str(len(aTags)) + \" articles to download\") #Send a status message saying how many articles have been located\n",
        "    \n",
        "    for a,aTag in enumerate(aTags): #For each article that we want to process\n",
        "        \n",
        "        if not(\"www.geeksforgeeks.org\" in aTag['href']): \n",
        "            continue #Don't follow external links\n",
        "        \n",
        "        with open(os.path.join(STORAGE_PATH,str(a)+\".html\"),\"w\") as f:\n",
        "            f.write(requests.get(aTag['href']).text) #Save the article for later processing.\n",
        "\n",
        "        time.sleep(SAFETY_DELAY) #Avoid DoS - as above\n",
        "        if a % 20 == 0: #Show progress every 20\n",
        "            print(a)\n",
        "\n",
        "    print(\"Articles saved to files\")\n",
        "\n",
        "ScrapeG4GResources()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "238 articles to download\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "120\n",
            "140\n",
            "160\n",
            "180\n",
            "200\n",
            "220\n",
            "Articles saved to files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SquQ9uwsGDua"
      },
      "source": [
        "def ProcessG4GResources():\n",
        "    \"\"\"Takes articles from G4G which are already saved locally and processes them into a set of machine learning features. The code is tagged according to the difficulty level that G4G has allocated it in order that a classifier might be able to predict this level from the provided code features\"\"\"\n",
        "    \n",
        "    LEVEL_STRINGS = [\"Basic\",\"Easy\",\"Medium\",\"Hard\",\"Expert\"] #The possible levels that G4G can allocate the resources\n",
        "    THINGS_TO_LOOK_FOR = [\"for\",\"if\",\"def\",\"lambda\",\"while\",\"await\",\"else\",\"elif\",\"import\",\"pass\",\"break\",\"except\",\"in\",\"raise\",\"class\",\"finally\",\"is\",\"return\",\"and\",\"continue\",\"try\",\"as\",\"from\",\"assert\",\"del\",\"global\",\"not\",\"with\",\"async\",\"or\",\"yield\",\"@\",\"print\"]#The number of times each of these appears in code on an article's page will be used as a feature. I will add more as necessary. \n",
        "    N_FEATURES = len(THINGS_TO_LOOK_FOR) + 5 + 1 #includes the output feature - the level\n",
        "    STORAGE_PATH = \"/content/drive/MyDrive/A Level/Computer Science/NEA/G4G Downloads\" #MAINTENANCE : This is the path where all of the G4G articles which have been downloaded are stored\n",
        "  \n",
        "    datasetMatrix = np.zeros((N_FEATURES,1)) #Initialise the matrix which will contain the data we collect\n",
        "    \n",
        "    for fNum,fileName in enumerate(os.listdir(STORAGE_PATH)): #For each article that we want to process\n",
        "        with open(os.path.join(STORAGE_PATH,fileName),\"r\") as f:\n",
        "            soup = bs4.BeautifulSoup(f.read()) #Create a new soup on the article page\n",
        "            classMeta = soup.find(\"div\",class_=\"meta\") #This reflects the structure of G4G - the difficulty level is a few HTML tags down inside a div with the class meta.\n",
        "            \n",
        "            if len(classMeta) == 0: #There isn't a difficulty associated with this article which is no good so skip it\n",
        "                continue\n",
        "\n",
        "            elif classMeta.findChild().findChild().findChild().getText() == \"Difficulty Level :\":\n",
        "                levelString = classMeta.findChild().findChild().findChildren()[1].findChild().getText() #Get the difficulty level assigned to the article\n",
        "\n",
        "                try:\n",
        "                    levelNumber = LEVEL_STRINGS.index(levelString) #Get a number to represent the level instead of the string (easier when developing ML algorithms)\n",
        "                except Exception as e: #Unrecognised level string - not in LEVEL_STRINGS\n",
        "                    print(\"Unrecognised level - we got this: \"+ str(e))\n",
        "                    continue \n",
        "                \n",
        "                codeBlockDivs = soup.find_all(\"td\",class_=\"code\") #Get all code blocks so we can process their contents\n",
        "                codeContents = \"\"            \n",
        "                for code in codeBlockDivs: #We will get all of the code on the page and put it together into one string variable\n",
        "                    for line in code.findChild().find_all(\"div\",recursive=False): #Separate lines of code are stored in separate divs so if we iterate over each div, grab the contents and append a new line we get the right full text of the article's code\n",
        "                        codeContents += line.getText() + \"\\n\"\n",
        "                    \n",
        "                thisPage = np.zeros((N_FEATURES,1)) #Initialise matrix for the features of this article\n",
        "                for i,item in enumerate(THINGS_TO_LOOK_FOR):\n",
        "                    thisPage[i][0] = codeContents.count(item)/len(codeContents.split(\"\\n\")) #One feature is the amount of times each of the THINGS_TO_LOOK_FOR appears in the code, on average per line \n",
        "\n",
        "                thisPage[i+1][0]= len(codeContents.split(\"\\n\")) #Number of lines\n",
        "                thisPage[i+2][0] = len(codeContents) #Total length\n",
        "                \n",
        "                totalCommentLength = 0\n",
        "                totalComments = 0\n",
        "                maxIndentDepth = 0 \n",
        "                for line in codeContents.split(\"\\n\"): #Go through all lines in the code\n",
        "                    if len(line) == 0: #Skip blank lines\n",
        "                        continue\n",
        "                    if line[0] == \"#\": #all g4g articles have comments on their own lines so we can just check if a hash appears at the start of the line.\n",
        "                        totalComments += 1\n",
        "                        totalCommentLength += len(line)\n",
        "                    if line[0] == \"\\xa0\" and line.count(\"\\xa0\") > maxIndentDepth: #Get indentation depth (\\xa0 is a non breaking space character, used to indent code by G4G)\n",
        "                        maxIndentDepth = line.count(\"\\xa0\")\n",
        "\n",
        "                #save calculated features to matrix\n",
        "                thisPage[i+3] = maxIndentDepth\n",
        "                thisPage[i+4] = totalCommentLength\n",
        "                thisPage[i+5] = totalComments\n",
        "                thisPage[i+6] = levelNumber\n",
        "                \n",
        "                #save this article's matrix to main one\n",
        "                datasetMatrix = np.hstack((datasetMatrix,thisPage))\n",
        "\n",
        "                print(fNum)\n",
        "\n",
        "    datasetMatrix = datasetMatrix[:,1:] #Skip the initial zero vector we still have at the front from intialisation\n",
        "    print(\"Finished processing \" + str(datasetMatrix.shape[1]) + \" articles\") #Report how many articles were processed\n",
        "\n",
        "    #return the matrix and the headers representing the meanings of the rows.\n",
        "    return datasetMatrix,','.join(THINGS_TO_LOOK_FOR+[\"noLines\",\"totalLength\",\"maxIndentDepth\",\"totalCommentLength\",\"totalComments\",\"levelNumber\"])\n",
        "\n",
        "np.set_printoptions(suppress=True) #Turn off scientific notation\n",
        "datasetMatrix,headers = ProcessG4GResources() #get data and headers\n",
        "\n",
        "print(\"Saving...\")\n",
        "np.savetxt(\"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GDatasetCSV.txt\", datasetMatrix.T, delimiter=\",\",fmt=\"%d\",header=headers) #Save as CSV with descriptive headers, no scientific notation, for import into other programs as necessary and for sharing\n",
        "print(\"Saved as CSV\")\n",
        "np.save(\"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GArray.npy\",datasetMatrix) #Save as numpy binary for later import into Python code\n",
        "print(\"Saved as NPY\")\n",
        "\n",
        "print(datasetMatrix[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z-epsTJlb5k"
      },
      "source": [
        "### Train and Use Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHC_vjnJj-_4"
      },
      "source": [
        "def tanh(matrix):\n",
        "    \"\"\"The hyperbolic tangent activation function. Used as the default activation for all layers except the last one here\"\"\"\n",
        "    return (np.exp(matrix) - np.exp(-matrix))/(np.exp(matrix)+np.exp(-matrix))\n",
        "\n",
        "def tanh_prime(matrix):\n",
        "    \"\"\"Derivative of the hyperbolic tangent activation function with respect to the input matrix\"\"\"\n",
        "    return 1-np.square(tanh(matrix))\n",
        "\n",
        "def softmax(matrix):\n",
        "    \"\"\"Computes the soft maximum vector-valued activation function on each column of a matrix. Used as the final layer activation since we are doing multiclass classification\"\"\"\n",
        "    temp = np.exp(matrix) #Compute elementwise e^n where e is Euler's number ~ 2.718\n",
        "    return temp/np.sum(temp,axis=0) #Normalise the result so that the columns sum to 1, by dividing through by columnwise sum\n",
        "\n",
        "def forwardProp(X,Ws,bs): \n",
        "    \"\"\"Performs 1 forward propagation through the network to get a predicted class (code level) for X (input features - each column is one training example so vectorised), Ws (list of weights), bs (list of biases)\"\"\"\n",
        "    As = [X] #Activations at each layer, with layer 0 having the input features as its activations\n",
        "    Zs = [] #Z values (input to the activation function === matrix product of Ws[current layer] and a[layer before] + bs[current layer])\n",
        "    \n",
        "    for i in range(len(Ws)-1): #For every layer in the network except the last one, which is dealt with separately as it has a different activation,\n",
        "        Zs.append(np.matmul(Ws[i],As[i]) + bs[i]) #Compute the next Z value by multiplying the corresponding weights with the previous layer's activations and adding the biases\n",
        "        As.append(tanh(Zs[-1])) #Compute this layer's activation by applying the tanh activation function\n",
        "\n",
        "    #Forward prop the final layer\n",
        "    Z_final = np.matmul(Ws[-1],As[-1]) + bs[-1] #Compute Z as usual \n",
        "    Zs.append(Z_final) #Save the final Z value\n",
        "    As.append(softmax(Z_final)) #Compute the softmax activation over the final logits and save as the final activations.\n",
        "    return Zs,As #Return the Z and Activation values, which will be needed for backprop \n",
        "\n",
        "def classifyCodeLevel(featureMatrix,Ws,bs):\n",
        "    \"\"\"Runs forward propagation on the network with the provided weights and input features and returns the final layer activations which is a matrix of probabilities for each possible classification\"\"\"\n",
        "    Zs, As = forwardProp(featureMatrix,Ws,bs)\n",
        "    return As[-1] #===y_hat\n",
        "\n",
        "def accuracyMetrics(X,Ws,bs,Y):\n",
        "    \"\"\"Compute some accuracy metrics for the Ws and bs that we have trained, against a given X-Y supervised learning data pair\"\"\"\n",
        "    print(X.shape)\n",
        "    Y_hat = classifyCodeLevel(X,Ws,bs) #Run forward propagation to get a prediction\n",
        "    print(Y)\n",
        "    print(Y_hat)\n",
        "    totalCorrect = np.sum(np.argmax(Y,axis=0) == np.argmax(Y_hat,axis=0)) #Predict the class with the maximum probablity in each case and check if it matches the expected one, producing a boolean matrix. Sum this matrix (True treated as 1, False as 0) to get the total number of correct ones.  \n",
        "    percentCorrect = totalCorrect/Y.shape[1]*100\n",
        "    diffs = np.abs(np.argmax(Y,axis=0) - np.argmax(Y_hat,axis=0)) #This variable is a matrix of values representing the absolute difference (number of classes) between the correct class value and the prediciton, for each input column. This is interesting in this case because the classes have an inherent order to them, meaning that if we classify say a Medium resoure as Easy that isn't as bad as classifying a Basic resource as Expert for example, so as well as the absolute percentage number of correctly predicted values, we are interested in how close we were when incorrect\n",
        "    \n",
        "    #Output what we have calculated\n",
        "    print(\"Y - ground truth labels\")\n",
        "    print(np.argmax(Y,axis=0))\n",
        "    print(\"Y-hat - predictions for these truth labels\")\n",
        "    print(np.argmax(Y_hat,axis=0))\n",
        "    print(\"% Accuracy\")\n",
        "    print(percentCorrect)\n",
        "    print(\"Absolute difference between Y and Y-hat in each case - smaller is better\")\n",
        "    print(diffs)\n",
        "\n",
        "def computeCost(X,Ws,bs,Y,lamd):\n",
        "    \"\"\"Computes the neural network cost function for the parameters Ws (weights), bs (biases) on input X and ground truth labels Y. lamd is the regularisation parameter lambda\"\"\"\n",
        "    Zs,As = forwardProp(X,Ws,bs) #Perform forward propagation to get the Zs and As. We will need to pass these back to the calling code for backprop\n",
        "    Y_hat = As[-1] #Classification predictions \n",
        "    m = Y.shape[1] #Number of training examples (needed as we average the cost over all training examples)\n",
        "    J = -1/m * np.sum(Y*np.log(Y_hat)) + lamd/(2*m)*sum([np.sum(np.square(w)) for w in Ws]) #Cross entropy loss for softmax regression multiway classification with Frobenius norm regularisation (results in a scalar value - the smaller the value the better the neural network is doing)\n",
        "    return J,Zs,As #Cost, Zs, Activations\n",
        "\n",
        "def train(X,Ws,bs,Y,iter,alpha,lamd):\n",
        "    \"\"\"Train a deep neural network with initial parameters Ws, bs, for iter iterations, with input features X and ground truth labels Y, with batch gradient descent (learning rate alpha) and weight decay regularisation parameter lamd\"\"\"\n",
        "    Js = [] #We will save the costs on each iteration to be able to plot them later to see that they are decreasing monotonically\n",
        "    m = Y.shape[1] #Number of training examples\n",
        "\n",
        "    for i in range(iter): #For each iteration, perform forward prop, get cost, perform backprop by gradient descent and update weights\n",
        "        cost,Zs,As = computeCost(X,Ws,bs,Y,lamd)\n",
        "        Js.append(cost)\n",
        "\n",
        "        dZ = As[-1] - Y #Derivative of the cost function with respect to the final layer logits Z\n",
        "        dWs = copy.deepcopy(Ws) #The derivatives of the cost function with respect to each layer's weights..\n",
        "        dbs = copy.deepcopy(bs) #...and biases. We use copy to initialise a dummy list of matrices with the correct dimensions although the values will be replaced - the W and dW values are not the same\n",
        "        \n",
        "        for backwardLayer in range(len(As)-2,-1,-1): #Move backward through the network, starting at the second to last set of activations\n",
        "            dWs[backwardLayer] = 1/m*np.matmul(dZ,As[backwardLayer].T) + lamd/m*Ws[backwardLayer] #Compute the partial derivative with respect to the cost function J of the weights at this layer\n",
        "            dbs[backwardLayer] = 1/m*np.sum(dZ,axis=1,keepdims=True) #Compute the partial derivative with respect to the cost function J of the biases at this layer\n",
        "            \n",
        "            if backwardLayer != 0: #If we are not back at the start of the network, then there is at least one more iteration (set of dW and db values to compute) and so we need to compute the parital derivative of J with respect to the next set of Z values, since this is used to compute the next layer's dW and db\n",
        "                dZ = np.matmul(Ws[backwardLayer].T,dZ)*tanh_prime(Zs[backwardLayer-1])\n",
        "\n",
        "        for i in range(len(Ws)): #Apply gradients via gradient descent to update the weights so that they hopefully get it right next time\n",
        "            Ws[i] = Ws[i] - alpha*dWs[i]\n",
        "            bs[i] = bs[i] - alpha*dbs[i]\n",
        "\n",
        "    plt.plot(Js) #debug plot of the cost function with iterations to check it is monotonically increasing\n",
        "    plt.title(\"Neural Network Cost Function with iterations of gradient descent\")\n",
        "    return Ws,bs #Trained weights and biases go back to calling code"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGshyC5G3NSw"
      },
      "source": [
        "DATA_LOCATION = \"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GArray.npy\" #The location of the feature array for the G4G articles dataset\n",
        "features = np.load(DATA_LOCATION)\n",
        "X = features[:-1,:]\n",
        "Y = features[-1,:].reshape((1,X.shape[1]))\n",
        "\n",
        "#Randomly re-order the dataset. (https://stackoverflow.com/questions/20546419/shuffle-columns-of-an-array-with-numpy) This is needed because the harder articles will tend to be towards the end of the data and I pick the first chunk to be training data then the rest to be test data and to get good results the two should come from the same distribution\n",
        "order = np.random.permutation(X.shape[1]) #Get a permutation of numbers which defines in what order we will put the dataset\n",
        "X = X[:, order] #Make sure to reorder both X and Y in the same way so that the labels still match their partner features.\n",
        "Y = Y[:, order]\n",
        "\n",
        "np.save(\"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GX.npy\",X) #Save randomly shuffled data as numpy binary for later import into Python code\n",
        "np.save(\"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GY.npy\",Y) #This is important as it means we can separately run this part of the code to reshuffle the dataset but we don't have to run it when training, meaning we can easily compare hyperparameter values as the training set is constant."
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asTSj1WnjsT9"
      },
      "source": [
        "X = np.load(\"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GX.npy\") #Load in the X and Y values from where they are stored\n",
        "Y = np.load(\"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GY.npy\")\n",
        "\n",
        "Y_POSS = 5 #There are 5 possible categories of output result (5 G4G levels)\n",
        "n_X = X.shape[0] #The number of input features.\n",
        "\n",
        "#Convert Y to a one-hot matrix, meaning that it is TRAINING_EXAMPLES x Y_POSS, with all zeros, ones where the row number represents the column's correct answer e.g. [0,1] becomes [[1,0,0,0,0],[0,1,0,0,0]]\n",
        "oneHotY = np.zeros((Y.shape[1],Y_POSS)) \n",
        "count = 0\n",
        "for y in Y[0]: #Existing Y is a 1xTRAINING_EXAMPLES matrix of values in the range 0 to 4\n",
        "    oneHotY[count][int(y)] = 1\n",
        "    count += 1\n",
        "oneHotY = oneHotY.T #We built it vertically but it needs to be horizontal based on the programming convention used here so transpose it\n",
        "\n",
        "#Normalise features to have 0 mean and 1 standard deviation\n",
        "mu = np.mean(X,axis=1).reshape(n_X,1)\n",
        "sigma = np.std(X,axis=1).reshape(n_X,1)\n",
        "for i in range(sigma.shape[0]):\n",
        "    if sigma[i,0] == 0:\n",
        "        sigma[i,0] = 1 #Some rows have 0 standard deviation which leads to division by zero error so just make the standard deviation 1 for now \n",
        "X -= mu\n",
        "X /= sigma\n",
        "\n",
        "#Split the dataset into train, cross-validation and test sets\n",
        "PERCENTAGE_TRAIN = 0.7 \n",
        "PERCENTAGE_CV = 0.15\n",
        "NUMBER_TRAIN = round(PERCENTAGE_TRAIN * X.shape[1])\n",
        "NUMBER_CV = round(PERCENTAGE_CV * X.shape[1])\n",
        "\n",
        "trainX = X[:,:NUMBER_TRAIN]\n",
        "cvX = X[:,NUMBER_TRAIN:NUMBER_TRAIN+NUMBER_CV]\n",
        "testX = X[:,NUMBER_TRAIN+NUMBER_CV:]\n",
        "\n",
        "trainY = oneHotY[:,:NUMBER_TRAIN]\n",
        "cvY = oneHotY[:,NUMBER_TRAIN:NUMBER_TRAIN+NUMBER_CV]\n",
        "testY = oneHotY[:,NUMBER_TRAIN+NUMBER_CV:]\n",
        "\n",
        "\n",
        "m = trainX.shape[1] #Number of examples in training set"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DUdWqG7dR_44",
        "outputId": "03a52323-38e8-4668-b19d-21aca2dad22a"
      },
      "source": [
        "NODES = [n_X,10,Y_POSS] #This describes the architecture of the network (number of nodes in each layer). The activations are set elsewhere\n",
        "\n",
        "#Randomly initialise weights and biases for each layer\n",
        "Ws = [] #Each value in this array is a numpy matrix representing the weights for the transition from a pair of consecutive layers. len(Ws) === len(nodes)-1\n",
        "for i in range(len(NODES)-1):\n",
        "    Ws.append(np.random.standard_normal((NODES[i+1],NODES[i])))\n",
        "bs = []#Each value in this array is a numpy matrix representing the biases for the transition from a pair of consecutive layers. len(bs) === len(nodes)-1\n",
        "for i in range(len(NODES)-1):\n",
        "    bs.append(np.random.standard_normal((NODES[i+1],1)))\n",
        "\n",
        "#MAINTENANCE: Network training constants - to improve performance of model try changing these and check train and cv set accuracy. Use test set to get a final unbiased estimate of how good the model is\n",
        "NUM_ITER = 50000 #How many epochs to train for\n",
        "ALPHA = 0.005 #Gradient descent learning rate\n",
        "LAMBDA = 5 #Regularisation parameter\n",
        "\n",
        "trainedWs,trainedbs = train(trainX,Ws,bs,trainY,NUM_ITER,ALPHA,LAMBDA) #Train the network\n",
        "print(\"\\nTrain set\")\n",
        "accuracyMetrics(trainX,trainedWs,trainedbs,trainY) #Compute accuracy metrics on the train set\n",
        "print(\"\\nCV set\")\n",
        "accuracyMetrics(cvX,trainedWs,trainedbs,cvY) #Compute accuracy metrics on the dev set"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train set\n",
            "(38, 153)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
            "  1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0.\n",
            "  0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
            "  1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1.\n",
            "  0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            "  1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1.\n",
            "  0. 1. 1. 1. 0. 1. 0. 1. 1.]\n",
            " [0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.\n",
            "  0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            "  0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
            "  0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0.07157121 0.082236   0.00271859 0.4748081  0.05767724 0.04230873\n",
            "  0.11132039 0.02214936 0.02997309 0.12935213 0.0255245  0.05631244\n",
            "  0.06996575 0.02428983 0.00906686 0.10060934 0.00542555 0.02424219\n",
            "  0.0402838  0.04340741 0.03575448 0.01445883 0.18009526 0.03326428\n",
            "  0.03338976 0.08005592 0.01090681 0.01714405 0.08649032 0.1499652\n",
            "  0.41837379 0.07971394 0.02030058 0.01357122 0.09967091 0.05936325\n",
            "  0.12544118 0.0301809  0.01180532 0.06243737 0.2067618  0.02921088\n",
            "  0.01327276 0.01561533 0.03527797 0.04046454 0.15417058 0.4592756\n",
            "  0.50267633 0.09013612 0.08009526 0.00910333 0.04908069 0.00910333\n",
            "  0.09471344 0.12544118 0.02998181 0.29398627 0.08215629 0.06963948\n",
            "  0.0129987  0.03189675 0.10184437 0.50267633 0.04023421 0.61423189\n",
            "  0.47167469 0.03091533 0.00505329 0.06762964 0.11824336 0.0269487\n",
            "  0.11857492 0.11280723 0.02405578 0.02658913 0.06524919 0.01006406\n",
            "  0.00448125 0.28631371 0.05088479 0.02206622 0.13791057 0.07971394\n",
            "  0.06524919 0.09456592 0.03931857 0.02214936 0.12544118 0.5419298\n",
            "  0.07843322 0.01195974 0.01884121 0.30734497 0.24491752 0.32391232\n",
            "  0.05320692 0.00785145 0.05716869 0.02247335 0.01094851 0.04058117\n",
            "  0.01946286 0.00957279 0.01031758 0.60676976 0.038351   0.39720651\n",
            "  0.18156393 0.01267296 0.04583169 0.32645466 0.12544118 0.00316139\n",
            "  0.03385102 0.13216926 0.02997727 0.04223353 0.03694856 0.06437713\n",
            "  0.03963154 0.18328625 0.06501884 0.40691971 0.02497077 0.04060989\n",
            "  0.12544118 0.07412229 0.15173454 0.01781205 0.12544118 0.03499661\n",
            "  0.21394108 0.02105717 0.24951551 0.02674033 0.11263446 0.00988968\n",
            "  0.02352963 0.01370497 0.03022081 0.03584748 0.02679706 0.10094643\n",
            "  0.3887136  0.04914149 0.05313385 0.02217996 0.12544118 0.05080263\n",
            "  0.45657395 0.02468391 0.08734702]\n",
            " [0.75781801 0.4503759  0.02013922 0.30378776 0.26232498 0.19419086\n",
            "  0.19074995 0.01767112 0.16445068 0.06136297 0.14291518 0.52953454\n",
            "  0.75568145 0.32541227 0.96932018 0.8411808  0.1369594  0.12091687\n",
            "  0.8882312  0.37126028 0.93229641 0.01716774 0.31743774 0.30426739\n",
            "  0.92124398 0.77375757 0.19438059 0.63558279 0.28754932 0.78023818\n",
            "  0.45050148 0.20397501 0.51646472 0.94361921 0.53289979 0.25831348\n",
            "  0.48152705 0.63976482 0.76317861 0.86615615 0.63990694 0.71644611\n",
            "  0.90502447 0.11457742 0.30771634 0.87964577 0.52841796 0.32741276\n",
            "  0.38459145 0.31759347 0.88554919 0.03014143 0.81354532 0.03014143\n",
            "  0.68639845 0.48152705 0.37413431 0.42316662 0.23520615 0.21593438\n",
            "  0.29868371 0.75242799 0.431119   0.38459145 0.81197629 0.26326372\n",
            "  0.46786395 0.31414448 0.11106594 0.76989052 0.60783187 0.08143356\n",
            "  0.84387442 0.70436829 0.92772181 0.89551412 0.544794   0.40652307\n",
            "  0.08139353 0.53104079 0.80805867 0.35034036 0.78710354 0.20397501\n",
            "  0.544794   0.64177837 0.18457782 0.01767112 0.48152705 0.4178983\n",
            "  0.33650125 0.9135899  0.86164767 0.21466738 0.69956229 0.55861478\n",
            "  0.12567501 0.10698499 0.90697066 0.90607529 0.86701212 0.22508012\n",
            "  0.15558405 0.12551831 0.18747974 0.26342994 0.85667966 0.42227704\n",
            "  0.70898143 0.91658105 0.88154411 0.43978953 0.48152705 0.01505968\n",
            "  0.41704457 0.21601379 0.23805531 0.08550751 0.44059789 0.73573867\n",
            "  0.9184945  0.52213108 0.20031857 0.56521863 0.89336696 0.66248014\n",
            "  0.48152705 0.80991471 0.83237376 0.15678113 0.48152705 0.78178602\n",
            "  0.58485204 0.45552435 0.68441405 0.11686557 0.35733689 0.91274549\n",
            "  0.51713409 0.80863905 0.52895746 0.04893754 0.15671058 0.81182459\n",
            "  0.53978779 0.74689282 0.59022938 0.32825285 0.48152705 0.63790938\n",
            "  0.51787017 0.51116055 0.77662939]\n",
            " [0.13994639 0.45644607 0.86942698 0.19092812 0.6618358  0.60846648\n",
            "  0.43650752 0.9131727  0.69855203 0.26123274 0.79430956 0.33377372\n",
            "  0.15555437 0.54748686 0.01191017 0.04990452 0.75830518 0.76208113\n",
            "  0.04421943 0.44104582 0.02839072 0.82799196 0.47832765 0.37804227\n",
            "  0.02980265 0.12582975 0.77460625 0.2494636  0.31902623 0.0288923\n",
            "  0.11027754 0.66518599 0.40824414 0.03848212 0.33307236 0.59794759\n",
            "  0.33333237 0.29426154 0.20918987 0.06153852 0.09859597 0.24037253\n",
            "  0.07650935 0.71620841 0.59812491 0.0722973  0.29984361 0.18738016\n",
            "  0.06064372 0.57513396 0.02595622 0.90277104 0.12722198 0.90277104\n",
            "  0.18099017 0.33333237 0.38397124 0.25515402 0.14426952 0.69083551\n",
            "  0.58498647 0.16478324 0.41386769 0.06064372 0.07230279 0.00955842\n",
            "  0.03948448 0.50248235 0.83022103 0.0211671  0.25509619 0.39893002\n",
            "  0.00433689 0.16878789 0.03915286 0.07244657 0.21434327 0.54468868\n",
            "  0.84864921 0.15623322 0.1013026  0.56249267 0.06019411 0.66518599\n",
            "  0.21434327 0.13994913 0.25221592 0.9131727  0.33333237 0.02578618\n",
            "  0.54047183 0.04643303 0.09204198 0.31307572 0.02543398 0.09249886\n",
            "  0.79253013 0.86954081 0.00219829 0.06570463 0.05824064 0.71885539\n",
            "  0.44179605 0.79676997 0.67796844 0.05026847 0.09770722 0.09207865\n",
            "  0.08164796 0.06702942 0.04025032 0.1866281  0.33333237 0.96755912\n",
            "  0.31957581 0.57583371 0.68552895 0.16351018 0.30356244 0.02457115\n",
            "  0.005314   0.24654719 0.51570559 0.01462943 0.06647458 0.12339325\n",
            "  0.33333237 0.07395    0.00576226 0.50949121 0.33333237 0.16785228\n",
            "  0.12472829 0.41636321 0.03296324 0.66584723 0.47862569 0.0745785\n",
            "  0.42434279 0.16742288 0.24663232 0.81928097 0.30837867 0.05688035\n",
            "  0.05117631 0.19593569 0.31038328 0.55749261 0.33333237 0.24675743\n",
            "  0.01395345 0.41960409 0.04770969]\n",
            " [0.02484992 0.00562127 0.10431165 0.01890878 0.01191254 0.14738462\n",
            "  0.21136978 0.04227732 0.09786038 0.53414958 0.03174912 0.07300001\n",
            "  0.01358098 0.09331461 0.00847103 0.00582458 0.0933379  0.0794297\n",
            "  0.02329636 0.13461264 0.00215561 0.1326394  0.01329689 0.26697965\n",
            "  0.01241964 0.01580166 0.01743883 0.09355739 0.28967716 0.03512767\n",
            "  0.01258183 0.04111318 0.05020524 0.00284594 0.02009277 0.06349285\n",
            "  0.04884492 0.03077638 0.01323029 0.00557544 0.04499057 0.00968822\n",
            "  0.00356045 0.14749426 0.04928191 0.00478651 0.00928466 0.01547992\n",
            "  0.04012737 0.00949639 0.00566471 0.05425807 0.00707431 0.05425807\n",
            "  0.0306573  0.04884492 0.1902826  0.01886487 0.52381695 0.01474051\n",
            "  0.09819218 0.04127645 0.04024489 0.04012737 0.06767197 0.10579363\n",
            "  0.01427532 0.14594357 0.04990379 0.12973115 0.01039431 0.47904913\n",
            "  0.02945154 0.00813104 0.00684691 0.00343968 0.15877581 0.03399089\n",
            "  0.06143732 0.01882905 0.03409811 0.05859572 0.00920099 0.04111318\n",
            "  0.15877581 0.11357451 0.51328087 0.04227732 0.04884492 0.00918139\n",
            "  0.03679667 0.02571176 0.02464353 0.13997785 0.02325126 0.01544124\n",
            "  0.02166295 0.01306115 0.03111141 0.00361158 0.05943839 0.01076507\n",
            "  0.3729966  0.06449622 0.11484668 0.06728176 0.00487009 0.07228759\n",
            "  0.01665114 0.00241105 0.02775608 0.03064292 0.04884492 0.01263798\n",
            "  0.21447934 0.06430915 0.03562758 0.69756042 0.21053204 0.16420654\n",
            "  0.03187522 0.03741261 0.19913194 0.00869725 0.01272434 0.16546196\n",
            "  0.04884492 0.03526442 0.00714233 0.30676896 0.04884492 0.01087345\n",
            "  0.05492009 0.10119374 0.0249994  0.18370962 0.04220134 0.00176194\n",
            "  0.0302449  0.00771773 0.18378548 0.08723556 0.49630127 0.02383712\n",
            "  0.01259833 0.0043674  0.0401966  0.08446365 0.04884492 0.05672165\n",
            "  0.00783066 0.03544384 0.08343465]\n",
            " [0.00581446 0.00532076 0.00340356 0.01156724 0.00624944 0.00764932\n",
            "  0.05005235 0.00472951 0.00916381 0.01390258 0.00550163 0.00737928\n",
            "  0.00521745 0.00949643 0.00123175 0.00248076 0.00597196 0.01333011\n",
            "  0.00396922 0.00967385 0.00140277 0.00774207 0.01084247 0.01744641\n",
            "  0.00314397 0.00455509 0.00266752 0.00425217 0.01725697 0.00577665\n",
            "  0.00826536 0.01001188 0.00478531 0.0014815  0.01426417 0.02088282\n",
            "  0.01085449 0.00501636 0.0025959  0.00429252 0.00974472 0.00428227\n",
            "  0.00163297 0.00610459 0.00959887 0.00280588 0.00828319 0.01045156\n",
            "  0.01196113 0.00764006 0.00273461 0.00372612 0.0030777  0.00372612\n",
            "  0.00724064 0.01085449 0.02163004 0.00882822 0.01455109 0.00885012\n",
            "  0.00513895 0.00961557 0.01292405 0.01196113 0.00781474 0.00715235\n",
            "  0.00670156 0.00651427 0.00375594 0.01158159 0.00843426 0.0136386\n",
            "  0.00376223 0.00590555 0.00222264 0.00201049 0.01683774 0.0047333\n",
            "  0.00403868 0.00758323 0.00565583 0.00650502 0.00559078 0.01001188\n",
            "  0.01683774 0.01013206 0.01060683 0.00472951 0.01085449 0.00520432\n",
            "  0.00779702 0.00230558 0.00282561 0.02493408 0.00683496 0.00953282\n",
            "  0.00692499 0.00256159 0.00255095 0.00213516 0.00436035 0.00471824\n",
            "  0.01016044 0.00364271 0.00938757 0.01225006 0.00239203 0.01615021\n",
            "  0.01115554 0.00130551 0.0046178  0.01648478 0.01085449 0.00158183\n",
            "  0.01504926 0.01167409 0.01081089 0.01118837 0.00835907 0.01110651\n",
            "  0.00468474 0.01062287 0.01982507 0.00453498 0.00246336 0.00805476\n",
            "  0.01085449 0.00674859 0.0029871  0.00914664 0.01085449 0.00449165\n",
            "  0.0215585  0.00586153 0.0081078  0.00683726 0.00920162 0.00102439\n",
            "  0.00474859 0.00251537 0.01040392 0.00869846 0.01181242 0.00651152\n",
            "  0.00772397 0.00366259 0.00605689 0.00761094 0.01085449 0.00780891\n",
            "  0.00377177 0.00910762 0.00487925]]\n",
            "Y - ground truth labels\n",
            "[1 2 2 2 2 2 4 2 2 3 2 2 1 2 1 1 2 2 1 2 1 2 2 3 1 0 2 1 3 1 1 2 1 1 1 2 2\n",
            " 1 1 1 1 1 1 2 2 1 1 0 0 2 1 2 1 2 1 3 3 1 3 2 2 1 1 0 1 0 0 2 2 1 2 3 1 0\n",
            " 1 1 1 1 2 0 1 2 1 2 1 1 3 2 1 0 2 1 1 0 1 1 2 2 1 1 1 2 3 2 2 0 1 0 1 1 1\n",
            " 1 1 2 2 0 2 3 3 1 1 0 2 1 1 1 2 1 1 2 1 1 1 1 1 2 1 1 2 1 1 2 3 1 0 1 1 1\n",
            " 2 1 0 1 1]\n",
            "Y-hat - predictions for these truth labels\n",
            "[1 2 2 0 2 2 2 2 2 3 2 1 1 2 1 1 2 2 1 2 1 2 2 2 1 1 2 1 2 1 1 2 1 1 1 2 1\n",
            " 1 1 1 1 1 1 2 2 1 1 0 0 2 1 2 1 2 1 1 2 1 3 2 2 1 1 0 1 0 0 2 2 1 1 3 1 1\n",
            " 1 1 1 2 2 1 1 2 1 2 1 1 3 2 1 0 2 1 1 2 1 1 2 2 1 1 1 2 2 2 2 0 1 1 1 1 1\n",
            " 1 1 2 1 2 2 3 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 2 3 1 1 1 1 2\n",
            " 1 1 1 1 1]\n",
            "% Accuracy\n",
            "82.35294117647058\n",
            "Absolute difference between Y and Y-hat in each case - smaller is better\n",
            "[0 0 0 2 0 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
            " 0 0 0 1 2 0 0 2 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
            " 1 0 1 0 0]\n",
            "\n",
            "CV set\n",
            "(38, 33)\n",
            "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1.\n",
            "  0. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0.12589669 0.09304646 0.14856443 0.06721882 0.05491294 0.02445933\n",
            "  0.02234361 0.00804615 0.12544118 0.3108899  0.43247656 0.71737808\n",
            "  0.00806169 0.21235509 0.07547917 0.16966022 0.04387128 0.05256475\n",
            "  0.01056443 0.03207097 0.02448922 0.00858036 0.63138098 0.12544118\n",
            "  0.02798653 0.18955515 0.02750128 0.01645317 0.11024097 0.34490526\n",
            "  0.67383113 0.63255837 0.07758147]\n",
            " [0.56151242 0.82218914 0.40117473 0.39687285 0.8219837  0.88611371\n",
            "  0.44757749 0.17997565 0.48152705 0.42788756 0.35581107 0.24372375\n",
            "  0.18874715 0.54219371 0.12011678 0.51267994 0.68728229 0.91370566\n",
            "  0.11740729 0.87121301 0.77480492 0.06593688 0.26005232 0.48152705\n",
            "  0.85157114 0.46604567 0.02398571 0.16400503 0.7125125  0.59072954\n",
            "  0.26698641 0.07170944 0.61259553]\n",
            " [0.23274428 0.02748173 0.33249999 0.45927296 0.01216602 0.00539406\n",
            "  0.46127702 0.77825697 0.33333237 0.24468109 0.18553098 0.01584162\n",
            "  0.7412302  0.193401   0.75787445 0.22594257 0.19343333 0.02738234\n",
            "  0.58054982 0.08863505 0.18692715 0.9075312  0.0782615  0.33333237\n",
            "  0.08419233 0.27333716 0.81390582 0.32051657 0.08952967 0.0535929\n",
            "  0.01862256 0.08915943 0.28514778]\n",
            " [0.06540259 0.04960512 0.08940089 0.06633797 0.10282383 0.07927128\n",
            "  0.06292129 0.03083691 0.04884492 0.0085372  0.01636687 0.01797173\n",
            "  0.05775137 0.0391302  0.03489842 0.07821212 0.06935728 0.00401709\n",
            "  0.28016345 0.00576044 0.01083514 0.01532553 0.02044777 0.04884492\n",
            "  0.03053204 0.05970837 0.1279781  0.48380981 0.07948789 0.0062825\n",
            "  0.03132915 0.18762896 0.01700914]\n",
            " [0.01444402 0.00767756 0.02835996 0.0102974  0.00811351 0.00476163\n",
            "  0.00588059 0.00288433 0.01085449 0.00800425 0.00981453 0.00508482\n",
            "  0.0042096  0.01292    0.01163118 0.01350515 0.00605582 0.00233017\n",
            "  0.011315   0.00232053 0.00294358 0.00262603 0.00985743 0.01085449\n",
            "  0.00571796 0.01135366 0.00662909 0.01521542 0.00822897 0.0044898\n",
            "  0.00923075 0.0189438  0.00766607]]\n",
            "Y - ground truth labels\n",
            "[1 1 1 3 0 1 2 1 1 1 1 0 2 1 3 2 2 1 1 2 0 2 1 1 4 1 1 1 3 1 1 1 1]\n",
            "Y-hat - predictions for these truth labels\n",
            "[1 1 1 2 1 1 2 2 1 1 0 0 2 1 2 1 1 1 2 1 1 2 0 1 1 1 2 3 1 1 0 0 1]\n",
            "% Accuracy\n",
            "48.484848484848484\n",
            "Absolute difference between Y and Y-hat in each case - smaller is better\n",
            "[0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 3 0 1 2 2 0 1 1 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTA3ewdUPjYp",
        "outputId": "1470e1b6-e4e7-45e2-a65c-ae40f0c68d07"
      },
      "source": [
        "np.save(\"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GWs.npy\",trainedWs) #Save weights as numpy binary for later import into Python code\n",
        "np.save(\"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GBs.npy\",trainedbs) #Save biases as numpy binary for later import into Python code"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjgiRy5jRDy8",
        "outputId": "4302456e-1d4e-49a4-cb2f-5834379d5614"
      },
      "source": [
        "#Example code for loading in ws and bs to be used in the actual IDE code\n",
        "w = np.load(\"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GWs.npy\",allow_pickle=True)\n",
        "b = np.load(\"/content/drive/MyDrive/A Level/Computer Science/NEA/G4GBs.npy\",allow_pickle=True)\n",
        "accuracyMetrics(testX,w,b,testY)"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(38, 33)\n",
            "[[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
            "  1. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.\n",
            "  0. 1. 1. 0. 0. 1. 1. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[0.12544118 0.02011117 0.34597002 0.37645655 0.25985926 0.00872265\n",
            "  0.01207029 0.01121697 0.09665483 0.16962947 0.09079426 0.03509265\n",
            "  0.00974889 0.02848146 0.3878219  0.03790906 0.12544118 0.12544118\n",
            "  0.00215501 0.12544118 0.04340741 0.0355398  0.00521946 0.13604695\n",
            "  0.27217854 0.0269736  0.05353331 0.20292417 0.00619449 0.04267244\n",
            "  0.06075957 0.06293292 0.01614787]\n",
            " [0.48152705 0.15721597 0.15681745 0.55422773 0.00983929 0.30088737\n",
            "  0.35733723 0.25974903 0.11009179 0.2798149  0.52857462 0.91030251\n",
            "  0.26267946 0.86587949 0.52354627 0.48974677 0.48152705 0.48152705\n",
            "  0.0247311  0.48152705 0.37126028 0.33208673 0.0057004  0.70026456\n",
            "  0.68166926 0.26449404 0.17703799 0.63772213 0.21269012 0.24053783\n",
            "  0.65696288 0.44514267 0.15099358]\n",
            " [0.33333237 0.7926843  0.05198469 0.0533602  0.33272693 0.64089147\n",
            "  0.54264967 0.65762181 0.73595287 0.47052446 0.29930616 0.0420404\n",
            "  0.70292251 0.09745493 0.02828569 0.31245057 0.33333237 0.33333237\n",
            "  0.92509642 0.33333237 0.44104582 0.37881861 0.93401141 0.14897381\n",
            "  0.03802278 0.10177112 0.65144831 0.09854666 0.63250354 0.68081067\n",
            "  0.2360458  0.4694217  0.71443467]\n",
            " [0.04884492 0.02428773 0.43117593 0.00978374 0.38636074 0.04466024\n",
            "  0.08300486 0.0653712  0.04742994 0.06674244 0.07038752 0.01012678\n",
            "  0.02190509 0.00544602 0.04720459 0.15133714 0.04884492 0.04884492\n",
            "  0.04552527 0.04884492 0.13461264 0.23649549 0.05228736 0.00802847\n",
            "  0.00481379 0.59479379 0.10967712 0.0507722  0.14117258 0.02728562\n",
            "  0.0401873  0.01284591 0.11223007]\n",
            " [0.01085449 0.00570083 0.01405192 0.00617177 0.01121377 0.00483826\n",
            "  0.00493796 0.00604099 0.00987057 0.01328873 0.01093745 0.00243766\n",
            "  0.00274405 0.0027381  0.01314156 0.00855645 0.01085449 0.01085449\n",
            "  0.0024922  0.01085449 0.00967385 0.01705937 0.00278137 0.00668621\n",
            "  0.00331563 0.01196745 0.00830326 0.01003484 0.00743927 0.00869344\n",
            "  0.00604445 0.00965681 0.00619381]]\n",
            "Y - ground truth labels\n",
            "[1 1 0 0 1 1 1 4 2 4 1 2 2 2 1 1 0 0 0 1 2 2 1 0 1 2 2 1 1 2 2 3 2]\n",
            "Y-hat - predictions for these truth labels\n",
            "[1 2 3 1 3 2 2 2 2 2 1 1 2 1 1 1 1 1 2 1 2 2 2 1 1 3 2 1 2 2 1 2 2]\n",
            "% Accuracy\n",
            "42.42424242424242\n",
            "Absolute difference between Y and Y-hat in each case - smaller is better\n",
            "[0 1 3 1 2 1 1 2 0 2 0 1 0 1 0 0 1 1 2 0 0 0 1 1 0 1 0 0 1 0 1 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HBFdc5-kM8N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
